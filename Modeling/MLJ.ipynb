{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLJ Example\n",
    "\n",
    "The closest equivalent I've seen to `scikit-learn` for Julia is `MLJ`. `MLJ` provides a unified API wrapper around a variety of ML libraries available in Julia. \n",
    "\n",
    "Here we'll walk through basic `MLJ` examples with some side notes on the quirks of this library.\n",
    "\n",
    "1. Selecting and loading a *'model'* in `MLJ`, where 'model' is a container of hyperparameters\n",
    "2. Create a *'machine'* which binds the *model* to *data* and checks data types\n",
    "3. `fit!` to fit the data\n",
    "4. `evaluate!` or `evaluate` the model performance using a *resampling* strategy against metrics (*measure*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pkg; Pkg.add(\"MLJ\"); Pkg.add(\"MLJDecisionTreeInterface\")\n",
    "using MLJ, DataFramesMeta, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTuple{(:sepal_length, :sepal_width, :petal_length, :petal_width, :target), Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Vector{Float64}, CategoricalArrays.CategoricalVector{String, UInt32, String, CategoricalArrays.CategoricalValue{String, UInt32}, Union{}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLJ has some datasets builtin\n",
    "iris = load_iris();\n",
    "typeof(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────┬─────────────┬──────────────┬─────────────┬──────────────────────────────────┐\n",
      "│\u001b[1m sepal_length \u001b[0m│\u001b[1m sepal_width \u001b[0m│\u001b[1m petal_length \u001b[0m│\u001b[1m petal_width \u001b[0m│\u001b[1m target                           \u001b[0m│\n",
      "│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m Float64      \u001b[0m│\u001b[90m Float64     \u001b[0m│\u001b[90m CategoricalValue{String, UInt32} \u001b[0m│\n",
      "│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Continuous   \u001b[0m│\u001b[90m Continuous  \u001b[0m│\u001b[90m Multiclass{3}                    \u001b[0m│\n",
      "├──────────────┼─────────────┼──────────────┼─────────────┼──────────────────────────────────┤\n",
      "│ 5.1          │ 3.5         │ 1.4          │ 0.2         │ setosa                           │\n",
      "│ 4.9          │ 3.0         │ 1.4          │ 0.2         │ setosa                           │\n",
      "│ 4.7          │ 3.2         │ 1.3          │ 0.2         │ setosa                           │\n",
      "└──────────────┴─────────────┴──────────────┴─────────────┴──────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# but it comes as a special NamedTuple with 'scitypes' inside of it, so let's make it into a dataframe\n",
    "iris = DataFrame(iris)\n",
    "# and we can `pretty` print the dataframe\n",
    "first(iris,3) |> pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a univariate time series one here too\n",
    "sunspots = MLJ.load_sunspots() |> DataFrame;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run an initial model on this DataFrame. Since `MLJ` is a wrapper on top of a bunch of other libraries, we need a way to explore the catalog of models that are available and load specific models so that we can put them to work. \n",
    "\n",
    "* See the [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/) section in the `MLJ` docs for more details\n",
    "* `models(\"KMeans\")`: Lists models in the catalog that match the name \"KMeans\"\n",
    "* `info(\"KMeans\", pkg=\"Clustering\")`: Show the parameters for the \"KMeans\" model from the \"Clustering\" package\n",
    "* `doc(\"KMeans\", pkg=\"Clustering\")`: Show the docs for the \"KMeans\" model from the \"Clustering\" package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┬──────────────┬───────────────────┬──────────────────────────────────┬───────────────┐\n",
      "│\u001b[1m name    \u001b[0m│\u001b[1m package_name \u001b[0m│\u001b[1m human_name        \u001b[0m│\u001b[1m hyperparameters                  \u001b[0m│\u001b[1m is_pure_julia \u001b[0m│\n",
      "│\u001b[90m String  \u001b[0m│\u001b[90m String       \u001b[0m│\u001b[90m String            \u001b[0m│\u001b[90m Tuple{Symbol, Symbol, Symbol}    \u001b[0m│\u001b[90m Bool          \u001b[0m│\n",
      "│\u001b[90m Textual \u001b[0m│\u001b[90m Textual      \u001b[0m│\u001b[90m Textual           \u001b[0m│\u001b[90m Tuple{Unknown, Unknown, Unknown} \u001b[0m│\u001b[90m Count         \u001b[0m│\n",
      "├─────────┼──────────────┼───────────────────┼──────────────────────────────────┼───────────────┤\n",
      "│ KMeans  │ Clustering   │ K-means clusterer │ (:k, :metric, :init)             │ true          │\n",
      "└─────────┴──────────────┴───────────────────┴──────────────────────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Show an example as a dataframe\n",
    "example = info(\"KMeans\", pkg=\"Clustering\") |> pairs |> collect |> DataFrame\n",
    "example[1,[\"name\", \"package_name\", \"human_name\", \"hyperparameters\", \"is_pure_julia\"]] |> DataFrame |> pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's an example of running the KMeans model with $k=3$ to split the `iris` data into 3 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(KMeans(k = 3, …), …).\n",
      "└ @ MLJBase /Users/nelsontang/.julia/packages/MLJBase/9Nkjh/src/machines.jl:492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8-element Vector{Tuple{CategoricalArrays.CategoricalValue{Int64, UInt32}, CategoricalArrays.CategoricalValue{String, UInt32}}}:\n",
       " (1, \"setosa\")\n",
       " (3, \"setosa\")\n",
       " (3, \"setosa\")\n",
       " (3, \"setosa\")\n",
       " (1, \"setosa\")\n",
       " (1, \"setosa\")\n",
       " (3, \"setosa\")\n",
       " (1, \"setosa\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KMeans = @load KMeans pkg=Clustering verbosity=0\n",
    "# Split the dataframe into y and X, where y is the column `target`  and X is everything else\n",
    "y, X = unpack(iris, ==(:target))\n",
    "model = KMeans(k=3)\n",
    "mach = machine(model, X) |> fit!\n",
    "\n",
    "yhat = predict(mach, X)\n",
    "\n",
    "# Check cluster assignments\n",
    "compare = zip(yhat, y) |> collect\n",
    "compare[1:8] # clusters align with classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review what happened in the cell above:\n",
    "\n",
    "* `@load`: loads the specified model from the catalog and brings it in to the environment\n",
    "* `unpack(iris, ==(:target))`: horizontally split the `iris` data into X and y, where y are all columns called \"target\" (here we use the symbol `:target`)\n",
    "* `KMeans(k=3)`: Instantiate the KMeans model (from the `@load` on line 1) and set $k=3$\n",
    "*  `machine(model, X) |> fit!`: We create a 'machine' that combines a model with training data and then `fit!` trains the model on the data\n",
    "   *  The pipe operator `|>` is similar to method chaining in python or `dplyr`'s pipe in R\n",
    "*  `predict(mach, X)`: Provides the predicted cluster assignments for each row of $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also created a fitted machine that we can call things like `fitted_params`, which in the case of `KMeans` just has a single parameter `centers` and each row corresponds to a column from the `iris` training data (sepal length, sepal width, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Matrix{Float64}:\n",
       " 5.21667  6.31458  4.74167\n",
       " 3.64     2.89583  2.95417\n",
       " 1.47333  4.97396  1.75417\n",
       " 0.28     1.70313  0.329167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitted_params(mach).centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can generate a report on the fitted machine with `report` to get some of the key outputs. This includes:\n",
    "\n",
    "* `assignments`: cluster assignments\n",
    "* `cluster_labels`: cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(assignments = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  3, 3, 1, 3, 3, 3, 1, 3, 3, 1],\n",
       " cluster_labels = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(mach)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more involved example with the `boston` dataset\n",
    "\n",
    "Let's build on the clustering example with an example from supervised learning. Below we'll take the popular boston housing dataset so we can show how to use `MLJ` to do things like a train/test split and evaluating against accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>3×14 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Crim</th><th style = \"text-align: left;\">Zn</th><th style = \"text-align: left;\">Indus</th><th style = \"text-align: left;\">Chas</th><th style = \"text-align: left;\">NOx</th><th style = \"text-align: left;\">Rm</th><th style = \"text-align: left;\">Age</th><th style = \"text-align: left;\">Dis</th><th style = \"text-align: left;\">Rad</th><th style = \"text-align: left;\">Tax</th><th style = \"text-align: left;\">PTRatio</th><th style = \"text-align: left;\">Black</th><th style = \"text-align: left;\">LStat</th><th style = \"text-align: left;\">MedV</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">0.00632</td><td style = \"text-align: right;\">18.0</td><td style = \"text-align: right;\">2.31</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.538</td><td style = \"text-align: right;\">6.575</td><td style = \"text-align: right;\">65.2</td><td style = \"text-align: right;\">4.09</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">296.0</td><td style = \"text-align: right;\">15.3</td><td style = \"text-align: right;\">396.9</td><td style = \"text-align: right;\">4.98</td><td style = \"text-align: right;\">24.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">0.02731</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">7.07</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.469</td><td style = \"text-align: right;\">6.421</td><td style = \"text-align: right;\">78.9</td><td style = \"text-align: right;\">4.9671</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: right;\">242.0</td><td style = \"text-align: right;\">17.8</td><td style = \"text-align: right;\">396.9</td><td style = \"text-align: right;\">9.14</td><td style = \"text-align: right;\">21.6</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">0.02729</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">7.07</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.469</td><td style = \"text-align: right;\">7.185</td><td style = \"text-align: right;\">61.1</td><td style = \"text-align: right;\">4.9671</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: right;\">242.0</td><td style = \"text-align: right;\">17.8</td><td style = \"text-align: right;\">392.83</td><td style = \"text-align: right;\">4.03</td><td style = \"text-align: right;\">34.7</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccccc}\n",
       "\t& Crim & Zn & Indus & Chas & NOx & Rm & Age & Dis & Rad & Tax & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Int64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.00632 & 18.0 & 2.31 & 0 & 0.538 & 6.575 & 65.2 & 4.09 & 1.0 & 296.0 & $\\dots$ \\\\\n",
       "\t2 & 0.02731 & 0.0 & 7.07 & 0 & 0.469 & 6.421 & 78.9 & 4.9671 & 2.0 & 242.0 & $\\dots$ \\\\\n",
       "\t3 & 0.02729 & 0.0 & 7.07 & 0 & 0.469 & 7.185 & 61.1 & 4.9671 & 2.0 & 242.0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×14 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Crim    \u001b[0m\u001b[1m Zn      \u001b[0m\u001b[1m Indus   \u001b[0m\u001b[1m Chas  \u001b[0m\u001b[1m NOx     \u001b[0m\u001b[1m Rm      \u001b[0m\u001b[1m Age     \u001b[0m\u001b[1m Dis     \u001b[0m\u001b[1m R\u001b[0m ⋯\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m F\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09      ⋯\n",
       "   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671\n",
       "   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671\n",
       "\u001b[36m                                                               6 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in boston housing market data\n",
    "boston = load_boston() |> DataFrame\n",
    "first(boston,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(\n",
       "  max_depth = -1, \n",
       "  min_samples_leaf = 1, \n",
       "  min_samples_split = 2, \n",
       "  min_purity_increase = 0.0, \n",
       "  n_subfeatures = -1, \n",
       "  n_trees = 10, \n",
       "  sampling_fraction = 0.7, \n",
       "  feature_importance = :impurity, \n",
       "  rng = Random._GLOBAL_RNG())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Test Split with `partition`\n",
    "train, test = partition(boston, 0.8, rng=42);\n",
    "\n",
    "# As before, unpack to horizontally split into y and X\n",
    "y_train, X_train = unpack(train, ==(:MedV))\n",
    "y_test, X_test = unpack(test, ==(:MedV));\n",
    "\n",
    "# Load in our model and instantiate it\n",
    "Tree = @load RandomForestRegressor pkg=DecisionTree verbosity=0;\n",
    "tree = Tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build our machine with the training data, and fit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(RandomForestRegressor(max_depth = -1, …), …).\n",
      "└ @ MLJBase /Users/nelsontang/.julia/packages/MLJBase/9Nkjh/src/machines.jl:492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: RandomForestRegressor(max_depth = -1, …)\n",
       "  args: \n",
       "    1:\tSource @108 ⏎ Table{Union{AbstractVector{Continuous}, AbstractVector{Count}}}\n",
       "    2:\tSource @321 ⏎ AbstractVector{Continuous}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mach = machine(tree, X_train, y_train) |> fit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Now we want to check our accuracy in terms of RMSE - instead of doing the old `predict` one time, let's use `evaluate` to see how this model performs with 5-fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 5 folds:  40%[==========>              ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 5 folds: 100%[=========================] Time: 0:00:00\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, operation, measurement, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_rows\n",
       "Extract:\n",
       "┌────────────────────────┬───────────┬─────────────┬─────────┬──────────────────\n",
       "│\u001b[22m measure                \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m 1.96*SE \u001b[0m│\u001b[22m per_fold       \u001b[0m ⋯\n",
       "├────────────────────────┼───────────┼─────────────┼─────────┼──────────────────\n",
       "│ RootMeanSquaredError() │ predict   │ 3.52        │ 0.4     │ [4.15, 3.16, 3. ⋯\n",
       "└────────────────────────┴───────────┴─────────────┴─────────┴──────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate!(mach, resampling=CV(nfolds=5, rng=42), measure=rms, operation=predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features = [:Crim, :Zn, :Indus, :Chas, :NOx, :Rm, :Age, :Dis, :Rad, :Tax, :PTRatio, :Black, :LStat],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(mach)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really cool - it lets you specify the evaluation strategy quite nicely. \n",
    "\n",
    "* See [measure list](https://alan-turing-institute.github.io/MLJ.jl/stable/performance_measures/#List-of-measures) for a list of possible performance measures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check RMSE against the test set with `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.423139873057788"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = predict(mach, X_test)\n",
    "# Check RMSE accuracy:\n",
    "rms(y_pred, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with `scitype`s\n",
    "\n",
    "The above example works as a quickstart but as soon as we jump into linear regression, we run into an issue. Let's say I load in Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressor(\n",
       "  fit_intercept = true, \n",
       "  solver = nothing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LinearRegressor = @load LinearRegressor pkg=MLJLinearModels verbosity=0\n",
    "ols = LinearRegressor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we run `ols_mach = machine(ols, X_train, y_train) |> fit!` again, we'll run into an error:\n",
    "\n",
    "```julia\n",
    "ols_mach = machine(ols, X_train, y_train) |> fit!\n",
    "\n",
    ">┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
    ">│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
    "..\n",
    ">│ In general, data in `machine(model, data...)` is expected to satisfy\n",
    ">│ \n",
    ">│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
    ">│ \n",
    ">│ In the present case:\n",
    ">│ \n",
    ">│ scitype(data) = Tuple{Table{Union{AbstractVector{Continuous}, AbstractVector{Count}}}, AbstractVector{Continuous}}\n",
    ">│ \n",
    ">│ fit_data_scitype(model) = Tuple{Table{<:AbstractVector{<:Continuous}}, AbstractVector{Continuous}}\n",
    "```\n",
    "\n",
    "The important part of this error is right after \"In the present case\" and it's the stuff in the `{}` brackets which specify the `scitype`.\n",
    "\n",
    "* `scitype(data) = ...` is telling us that we have `Continuous` and `Count` data\n",
    "* `fit_data_scitype(model) = ...` tells us that this model needs `Continuous` data only.\n",
    "\n",
    "In other words, the model needs `Continuous` data but there's `Count` data in there right now. We need to use `schema` to figure out where the problem is and `coerce` it into the right data type.\n",
    "\n",
    "### Check data `scitype` with `schema`\n",
    "\n",
    "You can inspect the data `scitype` with the `schema` command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬────────────┬─────────┐\n",
       "│\u001b[22m names   \u001b[0m│\u001b[22m scitypes   \u001b[0m│\u001b[22m types   \u001b[0m│\n",
       "├─────────┼────────────┼─────────┤\n",
       "│ Crim    │ Continuous │ Float64 │\n",
       "│ Zn      │ Continuous │ Float64 │\n",
       "│ Indus   │ Continuous │ Float64 │\n",
       "│ Chas    │ Count      │ Int64   │\n",
       "│ NOx     │ Continuous │ Float64 │\n",
       "│ Rm      │ Continuous │ Float64 │\n",
       "│ Age     │ Continuous │ Float64 │\n",
       "│ Dis     │ Continuous │ Float64 │\n",
       "│ Rad     │ Continuous │ Float64 │\n",
       "│ Tax     │ Continuous │ Float64 │\n",
       "│ PTRatio │ Continuous │ Float64 │\n",
       "│ Black   │ Continuous │ Float64 │\n",
       "│ LStat   │ Continuous │ Float64 │\n",
       "└─────────┴────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is `Chas`, which is dummy variable (0 or 1) indicating if the house is near the Charles River. According to our `schema` it's a `Count` (which is what `MLJ` does by default on integer data), and our model only takes `Continuous` so we'll need to `coerce` it to be `Continous`. \n",
    "\n",
    "* Reference: [MLJ Docs: scitypes and coercion](https://alan-turing-institute.github.io/MLJ.jl/dev/mlj_cheatsheet/#Scitypes-and-coercion)\n",
    "* Reference: [Data Science Tutorials in Julia: Linear Regression](https://juliaai.github.io/DataScienceTutorials.jl/isl/lab-3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬────────────┬─────────┐\n",
       "│\u001b[22m names   \u001b[0m│\u001b[22m scitypes   \u001b[0m│\u001b[22m types   \u001b[0m│\n",
       "├─────────┼────────────┼─────────┤\n",
       "│ Crim    │ Continuous │ Float64 │\n",
       "│ Zn      │ Continuous │ Float64 │\n",
       "│ Indus   │ Continuous │ Float64 │\n",
       "│ Chas    │ Continuous │ Float64 │\n",
       "│ NOx     │ Continuous │ Float64 │\n",
       "│ Rm      │ Continuous │ Float64 │\n",
       "│ Age     │ Continuous │ Float64 │\n",
       "│ Dis     │ Continuous │ Float64 │\n",
       "│ Rad     │ Continuous │ Float64 │\n",
       "│ Tax     │ Continuous │ Float64 │\n",
       "│ PTRatio │ Continuous │ Float64 │\n",
       "│ Black   │ Continuous │ Float64 │\n",
       "│ LStat   │ Continuous │ Float64 │\n",
       "└─────────┴────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = coerce(X_train, :Chas=>Continuous)\n",
    "X_test = coerce(X_test, :Chas=>Continuous)\n",
    "schema(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(LinearRegressor(fit_intercept = true, …), …).\n",
      "└ @ MLJBase /Users/nelsontang/.julia/packages/MLJBase/9Nkjh/src/machines.jl:492\n",
      "┌ Info: Solver: MLJLinearModels.Analytical\n",
      "│   iterative: Bool false\n",
      "│   max_inner: Int64 200\n",
      "└ @ MLJLinearModels /Users/nelsontang/.julia/packages/MLJLinearModels/Goxow/src/mlj/interface.jl:39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(coefs = [:Crim => -0.11324911498843833, :Zn => 0.04575441928605188, :Indus => 0.01945765891619942, :Chas => 2.0407623604133356, :NOx => -19.942391951773786, :Rm => 3.6700229309808057, :Age => -0.0004734356464829028, :Dis => -1.6863423449201385, :Rad => 0.32811707418244485, :Tax => -0.012908158787600524, :PTRatio => -1.0233533411398805, :Black => 0.008312853766953626, :LStat => -0.5495075409339456],\n",
       " intercept = 41.46800130672328,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ols_mach = machine(ols, X_train, y_train) |> fit!\n",
    "# See fitted parameters\n",
    "fitted_params(ols_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, operation, measurement, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_rows\n",
       "Extract:\n",
       "┌────────────────────────┬───────────┬─────────────┬─────────┬──────────────────\n",
       "│\u001b[22m measure                \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m 1.96*SE \u001b[0m│\u001b[22m per_fold       \u001b[0m ⋯\n",
       "├────────────────────────┼───────────┼─────────────┼─────────┼──────────────────\n",
       "│ RootMeanSquaredError() │ predict   │ 5.01        │ 0.53    │ [5.17, 5.58, 4. ⋯\n",
       "└────────────────────────┴───────────┴─────────────┴─────────┴──────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check RMSE\n",
    "evaluate!(ols_mach, resampling=CV(nfolds=5, rng=42), measure=rms, operation=predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check against the test set with `predict`\n",
    "\n",
    "Use `predict` to get predictions out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Test RMSE: 4.755466266615066"
     ]
    }
   ],
   "source": [
    "y_pred = predict(ols_mach, X_test);\n",
    "# Calculate RMSE\n",
    "ols_test_error = rms(y_pred, y_test)\n",
    "print(\"OLS Test RMSE: $ols_test_error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Data\n",
    "\n",
    "Now, what if we also wanted to standardize the data so we can run a regularized regression like Lasso or ElasticNet? `MLJ` contains some data transformation helpers that must be loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(Standardizer(features = Symbol[], …), …).\n",
      "└ @ MLJBase /Users/nelsontang/.julia/packages/MLJBase/9Nkjh/src/machines.jl:492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; caches model-specific representations of data\n",
       "  model: Standardizer(features = Symbol[], …)\n",
       "  args: \n",
       "    1:\tSource @263 ⏎ Table{AbstractVector{Continuous}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Standardizer = @load Standardizer pkg=MLJModels verbosity=0\n",
    "# Just like the other models, we instantiate it and bind data to it with a machine\n",
    "standardizer = Standardizer()\n",
    "standardizer_machine = machine(standardizer, X_train) |> fit!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use `transform` with this fitted machine to get data out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>5×13 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Crim</th><th style = \"text-align: left;\">Zn</th><th style = \"text-align: left;\">Indus</th><th style = \"text-align: left;\">Chas</th><th style = \"text-align: left;\">NOx</th><th style = \"text-align: left;\">Rm</th><th style = \"text-align: left;\">Age</th><th style = \"text-align: left;\">Dis</th><th style = \"text-align: left;\">Rad</th><th style = \"text-align: left;\">Tax</th><th style = \"text-align: left;\">PTRatio</th><th style = \"text-align: left;\">Black</th><th style = \"text-align: left;\">LStat</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">-0.107552</td><td style = \"text-align: right;\">-0.480605</td><td style = \"text-align: right;\">1.01981</td><td style = \"text-align: right;\">-0.292539</td><td style = \"text-align: right;\">-0.199694</td><td style = \"text-align: right;\">-0.73765</td><td style = \"text-align: right;\">-0.995154</td><td style = \"text-align: right;\">0.143208</td><td style = \"text-align: right;\">1.62477</td><td style = \"text-align: right;\">1.51459</td><td style = \"text-align: right;\">0.783051</td><td style = \"text-align: right;\">0.399715</td><td style = \"text-align: right;\">-0.316915</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">-0.0264694</td><td style = \"text-align: right;\">-0.480605</td><td style = \"text-align: right;\">1.01981</td><td style = \"text-align: right;\">-0.292539</td><td style = \"text-align: right;\">0.216848</td><td style = \"text-align: right;\">0.213194</td><td style = \"text-align: right;\">0.236529</td><td style = \"text-align: right;\">-0.439549</td><td style = \"text-align: right;\">1.62477</td><td style = \"text-align: right;\">1.51459</td><td style = \"text-align: right;\">0.783051</td><td style = \"text-align: right;\">0.404449</td><td style = \"text-align: right;\">0.232898</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">7.5345</td><td style = \"text-align: right;\">-0.480605</td><td style = \"text-align: right;\">1.01981</td><td style = \"text-align: right;\">-0.292539</td><td style = \"text-align: right;\">1.07597</td><td style = \"text-align: right;\">-0.462962</td><td style = \"text-align: right;\">1.12391</td><td style = \"text-align: right;\">-0.969986</td><td style = \"text-align: right;\">1.62477</td><td style = \"text-align: right;\">1.51459</td><td style = \"text-align: right;\">0.783051</td><td style = \"text-align: right;\">-3.56036</td><td style = \"text-align: right;\">1.10646</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">-0.236124</td><td style = \"text-align: right;\">-0.480605</td><td style = \"text-align: right;\">1.57981</td><td style = \"text-align: right;\">-0.292539</td><td style = \"text-align: right;\">0.598678</td><td style = \"text-align: right;\">-1.78428</td><td style = \"text-align: right;\">1.12391</td><td style = \"text-align: right;\">-1.1461</td><td style = \"text-align: right;\">-0.652551</td><td style = \"text-align: right;\">0.171736</td><td style = \"text-align: right;\">1.25087</td><td style = \"text-align: right;\">0.441581</td><td style = \"text-align: right;\">3.03081</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">-0.39497</td><td style = \"text-align: right;\">-0.480605</td><td style = \"text-align: right;\">-0.0573407</td><td style = \"text-align: right;\">-0.292539</td><td style = \"text-align: right;\">-1.23237</td><td style = \"text-align: right;\">-0.457327</td><td style = \"text-align: right;\">-1.80444</td><td style = \"text-align: right;\">0.719759</td><td style = \"text-align: right;\">-0.652551</td><td style = \"text-align: right;\">-0.602313</td><td style = \"text-align: right;\">0.31523</td><td style = \"text-align: right;\">0.231622</td><td style = \"text-align: right;\">-0.392271</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& Crim & Zn & Indus & Chas & NOx & Rm & Age & Dis & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & -0.107552 & -0.480605 & 1.01981 & -0.292539 & -0.199694 & -0.73765 & -0.995154 & 0.143208 & $\\dots$ \\\\\n",
       "\t2 & -0.0264694 & -0.480605 & 1.01981 & -0.292539 & 0.216848 & 0.213194 & 0.236529 & -0.439549 & $\\dots$ \\\\\n",
       "\t3 & 7.5345 & -0.480605 & 1.01981 & -0.292539 & 1.07597 & -0.462962 & 1.12391 & -0.969986 & $\\dots$ \\\\\n",
       "\t4 & -0.236124 & -0.480605 & 1.57981 & -0.292539 & 0.598678 & -1.78428 & 1.12391 & -1.1461 & $\\dots$ \\\\\n",
       "\t5 & -0.39497 & -0.480605 & -0.0573407 & -0.292539 & -1.23237 & -0.457327 & -1.80444 & 0.719759 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×13 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Crim       \u001b[0m\u001b[1m Zn        \u001b[0m\u001b[1m Indus      \u001b[0m\u001b[1m Chas      \u001b[0m\u001b[1m NOx       \u001b[0m\u001b[1m Rm        \u001b[0m\u001b[1m Age\u001b[0m ⋯\n",
       "     │\u001b[90m Float64    \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Flo\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ -0.107552   -0.480605   1.01981    -0.292539  -0.199694  -0.73765   -0. ⋯\n",
       "   2 │ -0.0264694  -0.480605   1.01981    -0.292539   0.216848   0.213194   0.\n",
       "   3 │  7.5345     -0.480605   1.01981    -0.292539   1.07597   -0.462962   1.\n",
       "   4 │ -0.236124   -0.480605   1.57981    -0.292539   0.598678  -1.78428    1.\n",
       "   5 │ -0.39497    -0.480605  -0.0573407  -0.292539  -1.23237   -0.457327  -1. ⋯\n",
       "\u001b[36m                                                               7 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_standardized = MLJ.transform(standardizer_machine, X_train)\n",
    "X_test_standardized = MLJ.transform(standardizer_machine, X_test)\n",
    "first(X_train_standardized, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(LassoRegressor(lambda = 1.0, …), …).\n",
      "└ @ MLJBase /Users/nelsontang/.julia/packages/MLJBase/9Nkjh/src/machines.jl:492\n",
      "┌ Info: Solver: MLJLinearModels.ProxGrad\n",
      "│   accel: Bool true\n",
      "│   max_iter: Int64 1000\n",
      "│   tol: Float64 0.0001\n",
      "│   max_inner: Int64 100\n",
      "│   beta: Float64 0.8\n",
      "└ @ MLJLinearModels /Users/nelsontang/.julia/packages/MLJLinearModels/Goxow/src/mlj/interface.jl:39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, operation, measurement, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_rows\n",
       "Extract:\n",
       "┌────────────────────────┬───────────┬─────────────┬─────────┬──────────────────\n",
       "│\u001b[22m measure                \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m 1.96*SE \u001b[0m│\u001b[22m per_fold       \u001b[0m ⋯\n",
       "├────────────────────────┼───────────┼─────────────┼─────────┼──────────────────\n",
       "│ RootMeanSquaredError() │ predict   │ 5.58        │ 0.528   │ [6.06, 6.05, 5. ⋯\n",
       "└────────────────────────┴───────────┴─────────────┴─────────┴──────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LassoRegressor = @load LassoRegressor pkg=MLJLinearModels verbosity=0\n",
    "\n",
    "lasso = LassoRegressor() #default λ=1\n",
    "lasso_mach = machine(lasso, X_train_standardized, y_train) |> fit!\n",
    "evaluate!(lasso_mach, resampling=CV(nfolds=5, rng=42), measure=rms, operation=predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with simple linear regression we can inspect the coefficients with `fitted_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(coefs = [:Crim => -0.0, :Zn => 0.0, :Indus => -0.0, :Chas => 0.0, :NOx => -0.0, :Rm => 2.4064540627537454, :Age => -0.0, :Dis => -0.0, :Rad => -0.0, :Tax => -0.0, :PTRatio => -1.6996069999970236, :Black => 0.23833314394555966, :LStat => -3.582731114009766],\n",
       " intercept = 22.287274937530785,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitted_params(lasso_mach)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like with the defaults it was able to do some basic feature engineering, but can we find a more optimal set of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoostRegressor = @load XGBoostRegressor pkg=XGBoost verbosity=0\n",
    "# xgb = XGBoostRegressor()\n",
    "# xgb_mach = machine(xgb, X_train_standardized, y_train) |> fit!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "Next, what if we want to try a *bunch of models* and use a grid search to find hyperparameters, similar to `GridSearchCV` in `scikit-learn`?\n",
    "\n",
    "In `MLJ`, we create a *Tuning Model* and specify *ranges* over which to search. \n",
    "\n",
    "With our `LassoRegressor` model above, we can take a look at the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoRegressor(\n",
       "  lambda = 1.0, \n",
       "  fit_intercept = true, \n",
       "  penalize_intercept = false, \n",
       "  scale_penalty_with_samples = true, \n",
       "  solver = nothing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default regularization strength lambda ($\\lambda$) is set to 1.0 - I want to try a range of different values of $\\lambda$ and see how it affects cross-validation RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training machine(DeterministicTunedModel(model = LassoRegressor(lambda = 1.0, …), …), …).\n",
      "└ @ MLJBase /Users/nelsontang/.julia/packages/MLJBase/9Nkjh/src/machines.jl:492\n",
      "┌ Info: Attempting to evaluate 10 models.\n",
      "└ @ MLJTuning /Users/nelsontang/.julia/packages/MLJTuning/ZFg3R/src/tuned_models.jl:727\n",
      "\r\u001b[33mEvaluating over 10 metamodels:  20%[=====>                   ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  30%[=======>                 ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  40%[==========>              ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  50%[============>            ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  60%[===============>         ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  70%[=================>       ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  80%[====================>    ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels:  90%[======================>  ]  ETA: 0:00:00\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[33mEvaluating over 10 metamodels: 100%[=========================] Time: 0:00:00\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trained Machine; does not cache data\n",
       "  model: DeterministicTunedModel(model = LassoRegressor(lambda = 1.0, …), …)\n",
       "  args: \n",
       "    1:\tSource @354 ⏎ Table{AbstractVector{Continuous}}\n",
       "    2:\tSource @600 ⏎ AbstractVector{Continuous}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter_range = range(lasso, :lambda, lower=0.001, upper=10.0, scale=:log)\n",
    "tuned_lasso = TunedModel(lasso, \n",
    "                         resampling=CV(nfolds=5, rng=42), \n",
    "                         tuning=Grid(resolution=10), # Search over 10 values between 'lower' and 'upper' in `parameter_range`\n",
    "                         range=parameter_range, \n",
    "                         measure=rms)\n",
    "# As before we then need to take this model and bind it to data as a machine\n",
    "tuned_lasso_machine = machine(tuned_lasso, X_train_standardized, y_train) |> fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = LassoRegressor(lambda = 0.02154434690031884, …),\n",
       " best_fitted_params = (coefs = [:Crim => -1.00621269776367, :Zn => 0.980277412099956, :Indus => 0.0, :Chas => 0.548151440210402, :NOx => -2.1972017021636727, :Rm => 2.6143569026062288, :Age => -0.0, :Dis => -3.3836879066403185, :Rad => 2.619535447511152, :Tax => -1.9458601239151805, :PTRatio => -2.158404705803287, :Black => 0.7705983695536748, :LStat => -3.923513035291781],\n",
       "                       intercept = 22.41855852728268,),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitted_params(tuned_lasso_machine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a *model* in `MLJ` is just a container of hyperparameters, we can get the best model from the above tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoRegressor(\n",
       "  lambda = 0.02154434690031884, \n",
       "  fit_intercept = true, \n",
       "  penalize_intercept = false, \n",
       "  scale_penalty_with_samples = true, \n",
       "  solver = nothing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lasso_model = fitted_params(tuned_lasso_machine).best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you use the Julia `Plots` library, there's some convenient stuff you can do with plotting a machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: scale log is unsupported with Plots.GRBackend().\n",
      "│ Choose from: [:identity, :ln, :log10, :log2]\n",
      "└ @ Plots /Users/nelsontang/.julia/packages/Plots/UTR4H/src/args.jl:1580\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARMAAAD6CAIAAAAN9zQRAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3deVwTVx4A8JcAkSAQCAECERERUU4VVEAsIIh31RbQiufqStv1PlBLV61HBbUere0qXkUtAoIHVfGglXKIFkVFTrkE5YYAgZCLJPvH7KYpIiZDJIn8vp/+wbyZefkx8utM3ryDIBaLEQBATkRlBwCAWoLMAQAPyBwA8IDMAQAPyBwA8IDMAQAPyBwA8IDMAQAPyBwA8MCTOenp6dbW1q9evVJ4NACoC00c54hEorKyMkNDQ0UF4e7ubm5urqGhoagK3yuRSEQkwr1aPmp00dra2mg02vnz53s+DE/mjB071sjI6Lfffps9ezau2LoqKSlZtmyZAlPxveJwOGQyWdlRqBk1umhZWVlPnjx552F4ModEIu3bt++LL76orKz86KOP9PT0pHcNGjQIR4UzZ840NzfHEUzfa2trk/6VgSxU8KKxWKwtB/5T2MhBCI001onY/AUWoZaWVnFx8TtPx5M5jY2NK1euRAitWbOmyy4nJ6dnz57hqBOAvlRXVzfpy935ntuQpQlCKKWtPm3ZV7/99G8TExMZa8CTOfr6+idOnOh2F5VKxVEhAH1s0/7j+T67kPb/b4N6JrleOzft//Hcwe0y1oAnc8hkMnbPAUBNlbJEf6UNhqxf0tIpew14MkdCIBBUVFRUV1cbGxvb2tqqS+MJAN0jEGQ/FuffOo/H27hxo4GBgY2NjZeXl52dnYWFxc8//4yvNgD6mJUeAfHa/1bEbRuqL0c64LznfP755+fOnZs7d+6UKVOoVOqrV6+io6OXLVumqam5cOFCfHUC0GcOhn7+ZNWOAs+vkK4RQgi1N41M2/vdT1/LXgMBxzwEVVVVFhYW33///apVqySFYrE4MDAwLy+voKBA3goZDEZWVha0Sn/AVPCitbS0hB44XtCAtUpr79/8hYGBAULo6tWrUVFRV65c6fl0PPecvLw8TU3NFStWSBcSCISQkJApU6bw+XwSiYSjWgD6koGBQeTerbhPx/M9R1tbu7Ozk8VidSlnMplaWlqamr1qdQBALeDJHFdXV319/X/+858tLS2SwtLS0rCwMB8fH2hhA/0BnvuDjo7ODz/8sGzZMktLy/Hjx2MtBA8fPqRQKIcOHVJ4iACoIJz3h0WLFmVkZMycObOysjI1NZXNZq9ZsyYnJ8fOzu5tp+Tk5AQGBrq4uKxYsaKhoQFvwACoBDyZU1JSsnXr1iFDhvzyyy+FhYXV1dVPnz49dOgQg8F42yk1NTXe3t6+vr5xcXFkMvmzzz7rRcwAKB+ezCkvL4+IiNDX15f9lJs3bw4fPvzzzz+3trb+7rvvMjIycnNzcXw0ACoCT+Y4ODhoaGiUlJTIfgqHw9HR0cF+1tLS0tLSgi7VQK3haSEwMzPbtm1bSEhIdHT0kCFDZDnF29s7NDT08ePHLi4uZ8+ebWtrq6urk+xlMplubm6SMaETJkw4fvw4jsD6BpvNJsjTwQkgtbpoXC5Xlu4BeDKHyWQmJSXl5eXZ2NhYWFhIjyywsbG5ePHim6c4ODj89NNPc+fOFQqFnp6eo0aNkh4IYWBgEBsba2pqim1SKBRdXV0cgfUNsVisyuGpJjW6aNra2rIkOZ7M0dDQGDp06NChQ9/c1cOA0KVLly5duhQhxGazzc3N7e3tJbuIRKKlpaW69L4BAOHLnAEDBoSHh5uZmck1svz169eDBg0SCASbNm0aNWrU6NGjcXw0ACoCTwtBSkqKtbV1e3v7uw+VMnnyZAaDYWBg8PLly0uXLuH4XABUB557juxjtaUVFBTU1taSyWQKhYLjdABUCp57zujRo0eNGnXq1Cl5T6TT6ZA24MOA557D5XI//vjjb7755v79+15eXtKvRKlUakBAgOLCA0BF4ckcFou1a9cuhND169evX78uvcvJyQkyB/QHeDLH2NiYyWR2u0tdZrgFoJfwZA6RSFSXmWwBeE/kaCEQCoWpqalvu9sghB49enT16lVFRAWAqpMjc9rb2728vDIzM7HNZ8+eEYlE6fk64uPjt2+XdYpEANQa/pHP4v9TYDQAqAuYMwAAPCBzAMADMgcAPORulU5PT2ez2QihiooKhNDt27cl46ILCwsVGxwAKkvuzAkPD5fe3LBhg/Smo6NjbyMCQB3IkTkDBw68c+dOz8eo2tzBALwncmSOpqbm5MmT318oAKgRaCEAAA857jlRUVE3btzo+RhLS8sDBw70LiQA1IAcmdPU1FRWVibZzMvL43K5JiYmhoaGNTU1LBbL2NjY19f3PQQJgMqR42ltw4YNj/5vypQpDg4OT58+raurKywsbGlp+fXXX7W0tIKDg99frACoDjzfc169ehUeHh4fH+/s7IyVEAiEmTNn7tmzZ+PGjQoNDwAVhSdz8vPzSSTSm1Or2djYFBcX8/l8RQQGgErDkzl0Op3L5SYmJnYpj42NNTIygqUOQX+AZ0yos7Ozn59fcHDw559/PmnSJCMjo6qqqpiYmISEhG+//VbhIQKggnCu6Xn58uW1a9ceO3bs8OHDWImhoeH+/fs3bdqkuNgAUF04M0dPT+/MmTNHjhwpLi5uaGgYPHjwsGHD4DkN9B+9WkdaX1/fxcVFUaEAoEbw975JTEycM2fOiBEjgoKCsJLNmzffvXtXQYEBoNJwZs6BAwdmz57d0NBgbGwsmZq9vb396NGjiosNANWFJ3OYTOa///3vvXv3ZmRkSG44CKFJkyY9fPhQcbEBoLrwZM6TJ0+EQmGXMW0IIQaD0dTUBG9CQX+As4Wg28miampqsNVzuz2ltbU1KiqqoqLCwsJiyZIlMEsoUGt47jmjRo0iEokxMTEIIcmKimKx+Pjx425ubt2uscjj8dzd3e/fv+/k5PTnn3+OHz+ew+H0Jm4AlAvPPcfIyGj16tUhISH5+fmtra0tLS3R0dGRkZFpaWm3b9/u9pS8vLyXL18+f/5cQ0Nj0aJFVCr12bNnbm5uvQseAKXB+bQWERFBIpGOHj2K3ToyMzNNTEyio6P9/Py6PX7w4MGampoFBQUODg5FRUUikcjKygp/1AAoG86Vp6qrq7dv375t27bs7OzGxkYGgzFmzJgBAwa87RQajRYXF+ft7U2j0err66OjoyVruCOE2traNm3apKOjg22OGDFi1apVOALrG1wu923f5cDbqNFFEwgEssz5jCdzUlJSpk2bVldXZ2Ji4u3tLcsptbW1y5cv37t3r4+PT1pa2sqVK7OyshgMBrZXS0vL2dlZshDi0KFDVfkq99AKAt5GjS6ahoZGt9/Vu8C/wq4stUskJiZaWFiEhIQghIYPH37+/PkrV65Ibiza2tqLFi0yNzfHEUzf09DQgAW25KVGF41IlKnZDOcKu87OzmfOnJH9FGwkAo/HQwgJBIJXr17RaDQcHw2AisC/wu6OHTuwFXZ1dXUlu962wu6sWbOOHj06btw4T0/PzMxMOp0+d+5c/FEDoGw4V9jdvXs3QigxMbHLyNC3rbBLIpH++OOPzMzMysrKBQsWuLu7y3hPBEA14ckcGo1WWlra7a4ehugQCAQPDw8PDw8cnwiAqsGTORoaGkOHDlV4KACoEXhkAgAPnH0IuFxuRETEnTt3ysvLuVyupNze3j4tLU1BsQGgunDecxYvXvztt99aW1uLRKKJEydOnz4dITRw4MBPP/1UoeEBoKLwZE5tbe2lS5eioqLOnTvHYDAWLFhw4cKF4uJiKpUqEAgUHiIAKghP5rx48UJDQwO7vRCJROxpzcjIaOfOnUeOHFFwgACoJJxPa0QiEXshY2pqWl1djRXS6fTq6mq47YD+AE/mDBs2rLOzs7y8HCE0atSo6Ojo1tZWoVB46tQpMzMzdenYB0Bv4Mkcc3NzDw+Pa9euIYS+/PLLqqoqY2NjIyOjM2fOhIaGKjpCAFQRzlbp9PR07Adzc/Ps7OyEhISGhgZfX19YSBT0E72a4xMzZMgQWDYH9DfQhwAAPPDccxoaGmxtbbvdBX0IQD+BJ3PIZPLKlSulS1gs1q1btwQCAfQhAP0EnszR1dUNDw/vUsjlcv39/Ts7OxURFQCqTmHfc7S1tdeuXXvgwAFFVQiAKlNkCwGJRGpqaoI+BKA/UECrNEJILBbn5eXt2rVr+PDh0IcA9AeKaVtjs9l8Pl9PTy86OlpBgQGg0hTTtjZw4EArK6tp06YZGRkpKDAAVJrC2tYA6FegDwEAeOCcb23z5s3vPGzbtm1DhgzBUT8Aqg9P5vD5/OTk5FevXgkEAgqFYmhoWFtby+Vy9fT0jI2NJYd9+eWXiosTANWC52mNRqMtXrzY3t4+KyurpaWlvLyczWYnJCTo6ur+9NNPpf/n7Oys8HABUBF47jlVVVW7d+8uLi6WrB5FJBI/+eST5ubmdevWFRQUKDRCAFQRnntOXl6elpaWpaVll3JbW9uioiJYmxr0B3gyx8TEhMvl3rhxo0v55cuXDQ0Ne5haGoAPBp6nNWdnZ29v788++2z16tWTJk0yNDSsrq6OjY2Njo7+5ptvFB4iAG8Si8UVFRV8Pt/a2lopa1rhyRwCgYCtuHbgwAHJK1F9ff3du3eHhYV1e0pnZ+ezZ8+kSxgMBp1Ox/HpAMTfuLv3l9tVBnYCjQFmzLPLvUZuXLm4j2PA2ePTwMDgwoUL33//fVFRUX19/eDBg21tbSVL5L6pvb0dW+oQ8+TJk8jIyOXLl+P7dNCf/Zn9dHVCbu2kfdhmCwraVXRL90JcyMKgvgyjV32lqVSqu7u7LEcaGBg8evQI+zk7O9vT0xNGjwJ8Is4m1Lr97dGGZTv1/L0dfZw58rUQ1NXVVVZWSpc8e/YsKCjI2dl56tSpSUlJslRy+vTpgIAAAwMDuT4aAEyzgIiIXb/YtHT29dgW+e45vr6+Dg4OMTEx2GZJScmECRM4HI6FhUVhYWFycvKtW7f8/Px6qIHL5V68ePHy5cvShXw+PzEx0dDQENscNGiQm5ubXIH1JaFQKBQKlR2FmlHgRSMTuqlHR0Nh9YtEIlkOkyNzGhsb8/Pz9+3bJynZu3cvh8NJSkry9/evra318vIKDw/vOXMSEhIMDAy8vLykC7lcbmJiIplMxjbHjBkzevRo2QPrY3w+H1tkG8hOgRdtrqfzH0/vsm3/mhNTo7HMzUJfUfULBAKxWPzOw+TInIqKCrFY7OTkhG2KxeKbN2/6+Pj4+/sjhOh0+qpVq/bs2dNzJWfOnFm+fDmBQJAu1NfXP3XqlLm5uezBKJFQKOyhLQR0S4EXbUVwYN7LI3EPS6uHTUOaJMPylEmEou8O7VTUYOQBAwZ0+fvslhyZ09HRgRDS09PDNouLi+vr6318fCQHDBs2rKGhgc/nv+1laHl5eWpq6s8//yz7hwLwpsNh69aUl1++8webw/34nx6jnPu6SRrJlTkWFhYIoZycHG9vb4TQ77//jhCaOHGi5ID6+nptbe0e+hCcOXPG398fqweA3rCystoYYqXEAOTInMGDB1tZWa1bt+748eMIoYMHD5qYmEiv0p6TkyPpA9otGxubOXPm4I4VANUhR+YQicTIyMhZs2Zh73A0NTXPnTunqfm/Gng83oULF+bNm9dDDYsXK+GuCsD7IF+rtJ+fX35+fmJiolAo9PPzk7QWIIRqa2s3bNgwc+ZMRUcIgCqSuw+BlZXV2rVr3yy3tLTcsmWLIkICQA3ADB4A4IEzc4qLiwMDA01MTAh/ByOoQT+Bp8cnl8v18fHhcDgLFy7s8vpSegYPAD5geDInNze3qqrqwYMH48ePV3hAAKgFPE9rLBaLQCCMGTNG4dEAoC7wZM64ceMoFMrDhw8VHg0A6gJP5ujo6Pz4448rVqy4du1aXV1dsxQWi6XwEAFQQThXAQkODkYIvdmVxsnJqct8AwB8kBS8lgG0rYF+Ak/mDBw4ELoLgH4O+hAAgAf+uW+eP3+enJz88uVLLpcrKWQwGNu3b1dEYACoNJyZc/z48X/96186OjrYQiCtra08Ho9CoUyYMEGx8QGgmvA8rbW3t2/atGn58uUtLS329vbff/89h8OJjY3V0dHZtm2bwkMEQAXhuee8ePGCzWaHh4dj8/l2dnYSCISgoKCampp169ZJZiQE4AOG557T1NREJpOpVCpCyMDAoLm5GSt3c3N78uRJZ2enIgMEQCXhyRwGg8HhcJqamhBCVlZW9+7dw8qfPn1KJpOVMrE8AH0Mz9PaiBEj6HT6zZs3Fy1atGzZMk9Pz5kzZ5qYmFy8eHHu3LmyzFUFgLrDc88hEok3b950cXFBCE2YMOHkyZONjY2pqamLFi368ccfFR0hAKoIZ6u09Oy1K1asWLFihYLiAUA99GoVED6fX1ZWpqmpOWzYMEUFBIBawNn7pq2tbdmyZQMHDhw5cuSaNWuwwilTphw+fFhxsQGgunBmTnBwcGJi4r59+7744gtJoZeXV3R0tIICA0Cl4cmc4uLiX3/9NS4ubtOmTSNHjpSUOzs7FxQUKC42AFQXnswpLS0dMGCA9CoGGAqFwmazYW0Z0B/gyRwKhcLj8bA3odJyc3OpVOqAAQMUERgAKg1P5ri4uJiYmGzdulUoFEree1ZWVu7bt2/GjBkKDQ8AFYWnVZpEIv3nP/8JCgq6f/++oaFhXV3dvHnzbt68qauru3fv3p7P5fF4sOYZ+ADgbFv75JNP0tLSrK2t8/LyysrKUlJSAgIC/vzzzx5WlWpqavrkk08oFIqJiQlMcQjUHf43oe7u7tevX5f9+MDAQCsrKyaTqaOjU1RUhPtzAVAFvepDILvHjx9nZ2cnJiZiz2m2trZ987kAvCdyZA6Pxzty5EjPxxgbG//jH/94szw3N3f48OGrV69OSkqiUCh79uwJDAyU7BWJRBUVFZL5DGg0mr6+vuyBAdD35MgcLpe7devWno9xcnLqNnPq6+uzsrKWLl169uzZlJSU6dOnOzs7Dx8+HNvb0tIyb948ycAed3f3EydOyB5YH2tvb1d2COpHjS4ah8MRiUTvPEyOzMFWyKHRaAsWLFiyZMmQIUPePOZtw9qMjY319fWxrjre3t4uLi737t2TZA6VSn3w4EGXBUVUmWRpeyA7dbloZDKZSHx3y5kcbWv6+vqVlZUbN268fv36mDFjJk+efOnSJS0tLUMpb3vKcnR0FAqFklTm8/laWlqyfzQAqka+VulBgwZt2bKlpKTk0aNHLi4uGzduNDExCQoKSk5OFovFPZzo4uLi6Oi4ffv22trac+fOFRUVTZ06tXeRA6BMON/nuLi4nDhxorq6+tixYzU1NZMnT96xY0fPp8THx5eWlnp7e8fGxt69e1eNns0AeFOvWqXb2toaGhqwDmwDBw7s+WAGgxETE9ObjwNAdeDJHB6Pd+fOnfPnz1+5csXY2DgwMDAmJsbJyUnhwQGgsuTLnMePH587dy46OrqtrW3y5MnR0dFz587V1Oyj16kAqA45/uhbW1tdXV319fUDAwPnz59vaGiIEOqyzhSZTLazs1NwjACoHrlvFywW6/Tp06dPn+52L6zZBt4pv6Dowq93mlvb/D1c5kz3V9MJ+uTIHDKZ/M5X+9iUuQC8zfo9h2NeadfaBSGLgT+nZo69uC7x2DcGBgbKjktucmQOiURauXLl+wsFfPBu3L13ptGCNXY2tsm18U5jjFq5/bu473crNzAc8LzPYbFYISEhb843kJqaumvXLkVEBT5MUdfvsew//luRjkFus5Ki6R08mcPhcCIjIwUCQZfy/Pz8hIQERUQFPkxcIQG98a2GI9aQpYelqlHkOqH19fVYgxsA3RpkSEac1i6FRhp8WXpYqhr52taioqLu37/P4XAQQmvWrJHutclkMm/duiU9cSEAXXz95ZLb6w6U+e1ChP+lim5x8meeI5QbFT7yZU5+fn5ycrJQKEQI3bt3T/p/FUZGRsuWLfvqq68UHCD4gJibmcV/vXjdd2FlyEQwQN+kvfwfPg7rVixWdlx4yJc5ERERERERDQ0Ntra2ubm57+yrBkAXox3t//j5YHt7e3t7O51OV3Y4+OHpOGNsbMxkMhUeCug/dHV1dXV1lR1Fr+DvclZXV3fhwoX8/PyqqipTU1Nvb+8FCxbABJ+gn8CZOenp6R9//HFzczONRqNSqenp6efOnTt8+HBycrKJiYliQwRABeFpDezs7AwODh40aFBWVlZDQ0NRURGLxYqPj6+oqNi0aZPCQwSqo7Oz8+nTp7fu/lZbW6vsWJQMzz0nOzu7srKysLBQMm0akUj89NNP6+vrQ0NDFRoeUCFJv6d9depasbE7R4dGj7nopd9yNvyrfvt8jidzmEymlpbWmyscjhw5sr29ncfj9dur+QGrqq7+8tSdl5P2YZvV1h6xLVWaYeHnDr5jFP2HCs/TmpWVlUAguHXrVpfy69evMxgMSJsP0pGzsS/H/Uu6RGTAeFAn7OzsVFZIyoXnnmNra+vj47NgwYItW7b4+/tTqdTXr19HR0dHRkZCj88P1avGFmRq1KWwjWzKZDL7Z5sQzra1mJiY4ODgsLCwsLCw/1WkqblmzZpt27YpLjagQuiGeqijBen8bSDNQE5Dv+2piDNzTExM7t69m5ubm5WVxWazaTTaRx99BBNBfcDWLw26HHbildeWv4ra6l2own4742SvJt9wcHBwcHBQVChAlVkOHnwwaNyu2B1FjEmdA42p1VmehJIzB/+t7LiUBn/mVFVVRUVF5efn19XVGRgY+Pn5LVq0CBZj+4AFzfKf5TcxI/Phq7piz8+8bGxWKDsiZcKZOSkpKbNnz25razM3N6fRaE+ePImPjz969Ojvv/+u1t34QM/IZLLfJG9lR6ES8LRKCwSChQsXDhs27NmzZ69fv3769GlDQ8P169dra2s3bNig8BCBolRUVDx8+LC5WT2HL6sYPPecx48fV1VVpaSkSF6GEgiEGTNmRERErF+/XiwWq+k8QB+wnLzCL8JPFA+0b9UxNW1JnaDfdvrbrfBo3Rt4MqelpUVLS+vN9XNsbGzYbDafz4eXoSqFzWbP3/mfAv/92EjMV2hqLKuOv3lPwo/fKjs0NYbnac3a2logENy4caNL+dWrVy0sLCBtVM25+MRCx39IBjAjhMT6po86KK2tXacEALLDc8+xsbGZPHnywoUL169fP2XKFBqN9vr16wsXLkRFRe3bt0/hIYJeynlRLrb4uEths/7QyspKR0dHpYT0AcDZtnbx4sUlS5bs3r179+7/zTFHIpE2b968efPmt52yc+fO/Px87GcGg3H48OFuD+vs7Iy8EJeRU2Soq7Ps0+kuznj+afl8/qno+Kz8UmMDvZD5s62HWuGoRCQS3f7tXsbTAruhFrOn+uEbOi4SidIzMp8UlTrbWE309HjbapA9Ky4t++5MbEMb14Km99Xni+Xt7WLFMEWtNcjIUrpQt73a1NQLRzAAgzNzjIyMrl+//uLFi0ePHrW3t5uamnp4eBgbG/dwyh9//OHq6jpu3DiE0NsWRXxdVTV7/b4c+2WddoFIwLl04tI80zvf79goV2wvikuDwo4+t18msglAnJaLe3/5fAwl7F/drPvbg+LSsuCvjz4dPEtAn0t4Vj3y8t69i/3mTJkkVyX5hS+W7vopx3wyz2g8qbDM8cTmE1v/4eIk37vjQ6cuHMqsr3JdhbS0Eac1ccMPB4MnfjLNT/YalgV+fHzt9+WTtv9VJOCMEFb0z/5mCiNWnKSkpBUrVrxtr7e3d0JCQre7zM3Nq6qqxGLx1H+Gop9Y6CRf8h9lTczt31PlCmPiovXoRId0JSbLf8grKJSrErfP1qDjbOlKrD/bxmQyxWIxi8WSpQahUDg6cBU6wfmrkkieY+BqPp8vexj19fWWi/dIh4FO8u3nb5KrErFYnHAz2T54G+GrDHTgJXndFc/gtVXV1XLV0EsyXjRVcOXKlTlz5rzzMLlbCLhc7oMHDxISEgoKCiSFv//+u7u7+7Rp0yTPY906dOjQjBkzQkND6+rq3twrEomKOwYgLW3pwlb72VG//iZ7eM3NzWWaDET827203vmzyNhE2SspLy8voTghjb/1yCob+VnMta4DK3qQ9ehxkfkkRJR6PCMQiixnpKRlyF7JtTv3KobO6FJYSp/w9OlT2StBCH0yzffhibB4t9Zvte/eCTBNu3DE3MxMrhpAF/I9rZWWlvr6+lZUVCCECARCaGhoWFjYF1988csvv5iZmR0+fDgkJORt586fP9/U1FRLS+v8+fPjxo3LycmhUCjYLiaT6ebmRiQSq4bP6noaUaOdK2hvb5cxwpqaGi7pjUdBbb16ZqvslVRUVLDIXZ9kxBR64cskbN6SHn5NiaKylx16g7sU8imD8oqz3MfJGkltIxNpd10Mna+lV1ffIPuvI+HvM9EfIYQQjnN7QyQS/fDDD2vXru3LD8WNy+WKe1wuGiNf5mzcuLGuru6rr74aPnz47du39+/fn5aWlpOTExERsXr1ajKZ3MO5kr+2adOm2dnZXb9+PTg4GCsxMDCIjY01NTWdue2Hgi6ntdQ4DKHLPsOQjY2NUXtM098LiTX5E0aPlL0SOzs72vno6r8XatYXebjaMZnMkydPbtz47q9eo+xH6Kc/Z1mOkS7UYZaMnTRC9kimTBh34FwWy+hvGWjWkD1+3FI1mnWpra0tPDxcMiBFxWlra8vyKl++p7X79++Hhobu3bt3yZIl0dHRHh4emZmZt2/fDg0N7Tlt/vaRRCKdTpd+mUAkEi0tLYcOHbrcd7RekdQTUSdv5MPvQkPkmANSS0truq0B6dXjv4oEHMfnp5bP/1T2SkxMTFy1alBb/V9FQoF9Uczc6f6yV+Lk6OjAzEQCzl9FnTz7qrvubuNkr2Ss6xj39j8JzNeSEq3q5/4m3J4bY0AfkOOew+FwGhsbx48fLylxc3NraGjw8PB457nt7e0vX77EhiQkJSU9fPjwh25J9lMAAAoYSURBVB9+ePOwjSsX616Ii7yzvUGDShJ2DCdzfjq4UfJQJ6NDX683PBp5KeVak7YZmd9ip8M5fvQrbW3td58pJfq77f/8en8Gc0AjdSSl/fXIzpcnD2wmkUhyVZJw+OulYXueEK0aDWyMWGWOvKKfv9sm7+zjV3/6dkvEsZT0ttZOIlWDP3uUxddhMAWxCpC9zaGlpQUhlJycLCkJCwtzcXGR5dza2lo6nW5kZGRqakqn06OioqT3StrWJNrb2zs7O2WPrVsNDQ0CgaA3NbBYrCdPnjQ0NEhK8vPzbW1t5ark1atXKX/8UVFR0ZtIxGJx7y+IsrBYLF1dXWVHISsZ29bkfp+TmpoqmRq3oKCgubn50qVLkr0UCsXfv5tHGlNT05qamsbGRpFI9OZrhJaWFg8PnG8J+5hAIKirq7O2tlZ2IOpELBZzOBx1uWgdHR00Gu2dhxHEMjQjYFpbW9+5niO+FXZLSkrUaAUVFov1tje54G3U6KKJRCIymcxgMHo+TI57jo6OTlxcXM/HyPudBPPm1G0AqDg57jmgZ3w+PzU1tbS01NbW1tvbW9nhqDQ2m33jxg0SiTRjxgw1nQNEbZ6RVN/p06evXbtGIBD27du3detWZYejusRi8dSpU0tKSu7fv79w4UJlh4MT3HMU7/Xr15MmTXrx4oWyA1FRKSkphw8fvnbtGkLIxcUlLi5OXRoPpME9R/Fu377t5uam7ChUV35+vmRckIODg3QHSDXSq/nW+qcnT5506Uji4OCwf/9+7GfsJe+dO3eUEZp64HK5knfKAwYM6OjoUG48+EDmdMXj8To6OrpM+srj8fLy8oyMjCwtLe3t7c+ePSu9V/J3kJ2dHRIScvXq1f489IXNZotEIj29v3VUbW5uLisrs7S0pNFo5ubmKSkpWHl1dfU7239V1Pt9H6tWnj175uzsrKmpSSKRpMtzcnIYDIa7u7upqem6det6ON3W1jYrK4vJZDY3N7//eFVOXFzcsGHDCATCxIkTpcvj4+OpVOrEiROpVGpUVBSTybS2tq6trS0qKrKxsellPw9lgcz5S3V1dUpKSmpqapfMmTZt2o4dO8RicW1tLY1Ge/ToUbennz171u//Zs+e3QcBq5rc3Nw///zz2LFj0pnD4/HodPqNGzfEYnFGRgaFQmlra0tOTvb3958xY0Z2drby4u0VyJyunj59Kp05LS0tRCKxsrIS21y6dGloaKiSQlMPkZGR0plz9+5dc3NzkUiEbdrZ2cXHxyspNEWCtrV3eP36tYaGxqBBg7BNKyuryspK5YakXiorK62srCQjXqysrLCRkeoOMucdOjo6SCSS5B9eW1ubzWYrNyT1gl1AySaZTFbTxrQuIHPewdTUlM1m83g8bLOpqQmmnJcLnU6XdK5HCDU2Nn4YFxAy5x0YDIa5uXlGxv+m3cjIyBg7dqxyQ1IvY8aMKSwsxKaB53K5jx49cnV1VXZQCqCxc+dOZcegKjo6Os6ePfvw4cO0tDQ6nV5UVOTo6EgkEgUCwf79+62srH755Ze7d+9GRkbKOzi0n6ioqIiLi0tPTy8oKNDW1sZanw0NDR8/fnzlyhUajbZz504KhbJly5Z316XyIHP+wuVyExMThUKhh4dHc3Mzl8udOHEiQsjDw4NAIMTExCCETp48aQbzLb1FTU1NSkqKnp6enZ1dc3Ozrq6uk5MTQmjmzJklJSWJiYnDhg07duyYvCPbVRP0+AQAD/ieAwAekDkA4AGZAwAekDkA4AGZAwAekDlKUF1dPW7cuN9+k2ONBtktWrQoNDRU9uOPHTs2aZJ86wIBBCPblILH42VlZTU1Nb37UPkVFhZyOJx3H/d/VVVV8q4pAhDccwDAB+45yicSiR49evTw4cPGxkYLC4uPPvpo+PDhkr2ZmZlsNtvHx+fKlSvPnz+3tLScP3++jo4Oi8W6dOlSVVWVq6vr9OnTu9TJZrMvXbpUWlo6bNiwoKAg6ZUmhELh5cuXsYGugYGBXU5sbm7+7bffiouLCQSCra3ttGnTPoxX/oqn7AFC/VFZWRlCKDY2FtvctWuXtra2s7Ozj48Pg8HQ0NA4efKk5OCAgABnZ+dp06ZZWFi4urpqamqOHz++rKxs6NChdnZ29vb2CKEtW7ZIjnd1dfX19bWzsxs5cqSHhweJRHJ0dGxqasL2crlcX19fIpE4duzY0aNHm5ubBwUFGRoaSk7X19c3MzPz9PR0c3PT0tKytbWtq6vrk6uiZiBzlKBL5uTm5kr+svl8/ooVK3R0dFpaWrCSgIAAhND69euFQqFYLL548SJCiE6nX7p0CTtg1apVJBJJ8veN9UT++uuvsc3MzExtbe2QkBBsMyIiAiEkWbD1zJkzCCHpzElNTZWM3ywsLDQyMlqzZs37uQzqDTJHCbpkThfY/GNpaWnYZkBAgK6uLofDwTaFQiGJRPL29pYc/+DBA4TQvXv3sE1XV1djY2NsyT7M0qVLKRQK9vPo0aO9vLykP2706NHSmdPF8uXL3dzc5Pz9+gVoIVC++vr6tWvXDh48WEtLi0AgjBw5EiEkPWbbyspK8mWDSCRSqdQRI0ZI9mJLVtTX/7XCnIODw4ABAySbLi4ura2t2AGlpaVdhse4uLhIbyYlJXl5eVGpVAKBQCAQTp8+/WEMflY4yBzlmzNnzpUrV8LDw7Ozs0tLS1NTUxFCAoFAckCX4UAEAkG6BBvpLZbq8y6dNgghrHmgo6NDJBLxeLwue6UbANLT02fMmGFpaXn16tUXL16UlpYuXry4s7NTIb/mBwba1pSsubn5wYMHJ0+eXLBgAVZSXl7eyzpLSkqkN4uLi0kkkoWFBbYe65t7JT/fvn3bwMAgKipKMu+C9K0MSIN7jpJhD83YYGOEkFAoPHjwYC/rLCkpSUpKwn5uaWk5d+6cl5cXtiSej49PYmLiy5cvsb35+fnJycmSE0UiEZ/Pl0xRkp2dfffu3V4G86GCe46SUanUKVOm7Ny5s66uztjY+MqVK5qavf1HcXBwWLRo0eLFi42MjC5cuMBisSTTXm/fvv3atWuenp7Lly8XCoUnT54cPXp0aWkptjcgIODAgQO+vr7z5s2rqqo6e/asq6sr1p4BuoDR1EogEolaW1v9/PzMzc0RQrNmzeLxeJmZmaWlpTNnzoyIiGhtbfX19bWwsEAINTc3W1pafvTRR5LT6+rqxo8fjy30jRASCoXStTU2Nnp7e2/ZsuX27dv379+3s7M7e/bsqFGjsIP19PQCAgKqqqrS09M7OjrCw8Pd3NwMDQ2nTp2KEDIzM/Py8iooKMjIyNDU1Dxy5IiTk5ORkVG3a7/2czCaGgA84HsOAHhA5gCAB2QOAHhA5gCAB2QOAHhA5gCAB2QOAHhA5gCAB2QOAHhA5gCAB2QOAHhA5gCAB2QOAHhA5gCAB2QOAHhA5gCAB2QOAHj8F8i9BuLvp7vWAAAAAElFTkSuQmCC",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"275\" height=\"250\" viewBox=\"0 0 1100 1000\">\n<defs>\n  <clipPath id=\"clip150\">\n    <rect x=\"0\" y=\"0\" width=\"1100\" height=\"1000\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip150)\" d=\"\nM0 1000 L1100 1000 L1100 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip151\">\n    <rect x=\"220\" y=\"10\" width=\"771\" height=\"771\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip150)\" d=\"\nM178.406 782.583 L1052.76 782.583 L1052.76 47.2441 L178.406 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip152\">\n    <rect x=\"178\" y=\"47\" width=\"875\" height=\"736\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  409.366,782.583 409.366,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  821.795,782.583 821.795,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,782.583 1052.76,782.583 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,47.2441 1052.76,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  409.366,782.583 409.366,763.685 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  821.795,782.583 821.795,763.685 \n  \"/>\n<path clip-path=\"url(#clip150)\" d=\"M359.632 846.308 L367.271 846.308 L367.271 819.942 L358.961 821.609 L358.961 817.349 L367.224 815.683 L371.9 815.683 L371.9 846.308 L379.539 846.308 L379.539 850.243 L359.632 850.243 L359.632 846.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M398.984 818.761 Q395.372 818.761 393.544 822.326 Q391.738 825.868 391.738 832.997 Q391.738 840.104 393.544 843.669 Q395.372 847.21 398.984 847.21 Q402.618 847.21 404.423 843.669 Q406.252 840.104 406.252 832.997 Q406.252 825.868 404.423 822.326 Q402.618 818.761 398.984 818.761 M398.984 815.058 Q404.794 815.058 407.849 819.664 Q410.928 824.248 410.928 832.997 Q410.928 841.724 407.849 846.331 Q404.794 850.914 398.984 850.914 Q393.173 850.914 390.095 846.331 Q387.039 841.724 387.039 832.997 Q387.039 824.248 390.095 819.664 Q393.173 815.058 398.984 815.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M410.928 809.159 L435.04 809.159 L435.04 812.356 L410.928 812.356 L410.928 809.159 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M446.512 819.635 L459.772 819.635 L459.772 822.832 L441.942 822.832 L441.942 819.635 Q444.105 817.397 447.829 813.635 Q451.572 809.855 452.531 808.764 Q454.355 806.714 455.07 805.304 Q455.803 803.874 455.803 802.501 Q455.803 800.263 454.223 798.852 Q452.662 797.442 450.142 797.442 Q448.355 797.442 446.362 798.063 Q444.387 798.683 442.13 799.943 L442.13 796.107 Q444.425 795.185 446.418 794.715 Q448.412 794.245 450.067 794.245 Q454.43 794.245 457.026 796.426 Q459.621 798.608 459.621 802.257 Q459.621 803.987 458.963 805.548 Q458.324 807.09 456.612 809.197 Q456.142 809.742 453.622 812.356 Q451.101 814.952 446.512 819.635 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M786.778 846.308 L794.417 846.308 L794.417 819.942 L786.107 821.609 L786.107 817.349 L794.371 815.683 L799.047 815.683 L799.047 846.308 L806.686 846.308 L806.686 850.243 L786.778 850.243 L786.778 846.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M826.13 818.761 Q822.519 818.761 820.69 822.326 Q818.885 825.868 818.885 832.997 Q818.885 840.104 820.69 843.669 Q822.519 847.21 826.13 847.21 Q829.764 847.21 831.57 843.669 Q833.398 840.104 833.398 832.997 Q833.398 825.868 831.57 822.326 Q829.764 818.761 826.13 818.761 M826.13 815.058 Q831.94 815.058 834.996 819.664 Q838.074 824.248 838.074 832.997 Q838.074 841.724 834.996 846.331 Q831.94 850.914 826.13 850.914 Q820.32 850.914 817.241 846.331 Q814.186 841.724 814.186 832.997 Q814.186 824.248 817.241 819.664 Q820.32 815.058 826.13 815.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M847.779 797.254 Q844.845 797.254 843.359 800.15 Q841.892 803.028 841.892 808.821 Q841.892 814.595 843.359 817.491 Q844.845 820.369 847.779 820.369 Q850.732 820.369 852.199 817.491 Q853.685 814.595 853.685 808.821 Q853.685 803.028 852.199 800.15 Q850.732 797.254 847.779 797.254 M847.779 794.245 Q852.5 794.245 854.983 797.987 Q857.484 801.711 857.484 808.821 Q857.484 815.911 854.983 819.654 Q852.5 823.378 847.779 823.378 Q843.058 823.378 840.557 819.654 Q838.074 815.911 838.074 808.821 Q838.074 801.711 840.557 797.987 Q843.058 794.245 847.779 794.245 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M499.486 876.846 L505.343 876.846 L505.343 926.372 L499.486 926.372 L499.486 876.846 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M533.797 908.452 Q526.7 908.452 523.962 910.075 Q521.225 911.699 521.225 915.614 Q521.225 918.733 523.262 920.579 Q525.331 922.393 528.864 922.393 Q533.734 922.393 536.662 918.956 Q539.622 915.486 539.622 909.757 L539.622 908.452 L533.797 908.452 M545.478 906.033 L545.478 926.372 L539.622 926.372 L539.622 920.961 Q537.617 924.207 534.625 925.767 Q531.633 927.295 527.304 927.295 Q521.83 927.295 518.583 924.239 Q515.369 921.152 515.369 915.995 Q515.369 909.98 519.379 906.924 Q523.421 903.869 531.41 903.869 L539.622 903.869 L539.622 903.296 Q539.622 899.254 536.948 897.057 Q534.307 894.829 529.501 894.829 Q526.445 894.829 523.549 895.562 Q520.652 896.294 517.979 897.758 L517.979 892.347 Q521.193 891.106 524.217 890.501 Q527.241 889.864 530.105 889.864 Q537.84 889.864 541.659 893.875 Q545.478 897.885 545.478 906.033 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M585.296 897.567 Q587.492 893.62 590.548 891.742 Q593.603 889.864 597.741 889.864 Q603.311 889.864 606.335 893.779 Q609.358 897.662 609.358 904.855 L609.358 926.372 L603.47 926.372 L603.47 905.046 Q603.47 899.922 601.656 897.439 Q599.842 894.957 596.118 894.957 Q591.566 894.957 588.924 897.981 Q586.283 901.004 586.283 906.224 L586.283 926.372 L580.394 926.372 L580.394 905.046 Q580.394 899.89 578.58 897.439 Q576.766 894.957 572.978 894.957 Q568.491 894.957 565.849 898.012 Q563.207 901.036 563.207 906.224 L563.207 926.372 L557.319 926.372 L557.319 890.724 L563.207 890.724 L563.207 896.262 Q565.212 892.983 568.013 891.424 Q570.814 889.864 574.665 889.864 Q578.548 889.864 581.254 891.838 Q583.991 893.811 585.296 897.567 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M646.63 908.579 Q646.63 902.118 643.956 898.458 Q641.314 894.766 636.667 894.766 Q632.02 894.766 629.347 898.458 Q626.705 902.118 626.705 908.579 Q626.705 915.041 629.347 918.733 Q632.02 922.393 636.667 922.393 Q641.314 922.393 643.956 918.733 Q646.63 915.041 646.63 908.579 M626.705 896.134 Q628.551 892.952 631.352 891.424 Q634.185 889.864 638.1 889.864 Q644.593 889.864 648.635 895.02 Q652.709 900.177 652.709 908.579 Q652.709 916.982 648.635 922.138 Q644.593 927.295 638.1 927.295 Q634.185 927.295 631.352 925.767 Q628.551 924.207 626.705 921.024 L626.705 926.372 L620.817 926.372 L620.817 876.846 L626.705 876.846 L626.705 896.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M685.874 896.134 L685.874 876.846 L691.731 876.846 L691.731 926.372 L685.874 926.372 L685.874 921.024 Q684.028 924.207 681.195 925.767 Q678.394 927.295 674.448 927.295 Q667.987 927.295 663.912 922.138 Q659.87 916.982 659.87 908.579 Q659.87 900.177 663.912 895.02 Q667.987 889.864 674.448 889.864 Q678.394 889.864 681.195 891.424 Q684.028 892.952 685.874 896.134 M665.918 908.579 Q665.918 915.041 668.559 918.733 Q671.233 922.393 675.88 922.393 Q680.527 922.393 683.201 918.733 Q685.874 915.041 685.874 908.579 Q685.874 902.118 683.201 898.458 Q680.527 894.766 675.88 894.766 Q671.233 894.766 668.559 898.458 Q665.918 902.118 665.918 908.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M719.994 908.452 Q712.897 908.452 710.159 910.075 Q707.422 911.699 707.422 915.614 Q707.422 918.733 709.459 920.579 Q711.528 922.393 715.061 922.393 Q719.931 922.393 722.859 918.956 Q725.819 915.486 725.819 909.757 L725.819 908.452 L719.994 908.452 M731.675 906.033 L731.675 926.372 L725.819 926.372 L725.819 920.961 Q723.814 924.207 720.822 925.767 Q717.83 927.295 713.501 927.295 Q708.027 927.295 704.78 924.239 Q701.566 921.152 701.566 915.995 Q701.566 909.98 705.576 906.924 Q709.618 903.869 717.607 903.869 L725.819 903.869 L725.819 903.296 Q725.819 899.254 723.145 897.057 Q720.504 894.829 715.697 894.829 Q712.642 894.829 709.746 895.562 Q706.849 896.294 704.176 897.758 L704.176 892.347 Q707.39 891.106 710.414 890.501 Q713.438 889.864 716.302 889.864 Q724.037 889.864 727.856 893.875 Q731.675 897.885 731.675 906.033 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  178.406,762.579 1052.76,762.579 \n  \"/>\n<polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  178.406,601.575 1052.76,601.575 \n  \"/>\n<polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  178.406,440.572 1052.76,440.572 \n  \"/>\n<polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  178.406,279.568 1052.76,279.568 \n  \"/>\n<polyline clip-path=\"url(#clip152)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  178.406,118.565 1052.76,118.565 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,782.583 178.406,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1052.76,782.583 1052.76,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,762.579 197.303,762.579 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,601.575 197.303,601.575 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,440.572 197.303,440.572 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,279.568 197.303,279.568 \n  \"/>\n<polyline clip-path=\"url(#clip150)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  178.406,118.565 197.303,118.565 \n  \"/>\n<path clip-path=\"url(#clip150)\" d=\"M141.003 745.299 L159.359 745.299 L159.359 749.234 L145.285 749.234 L145.285 757.706 Q146.304 757.359 147.323 757.197 Q148.341 757.012 149.36 757.012 Q155.147 757.012 158.526 760.183 Q161.906 763.354 161.906 768.771 Q161.906 774.35 158.434 777.452 Q154.961 780.53 148.642 780.53 Q146.466 780.53 144.198 780.16 Q141.952 779.789 139.545 779.049 L139.545 774.35 Q141.628 775.484 143.85 776.04 Q146.073 776.595 148.549 776.595 Q152.554 776.595 154.892 774.489 Q157.23 772.382 157.23 768.771 Q157.23 765.16 154.892 763.053 Q152.554 760.947 148.549 760.947 Q146.674 760.947 144.799 761.364 Q142.948 761.78 141.003 762.66 L141.003 745.299 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M150.378 599.712 Q147.23 599.712 145.378 601.865 Q143.549 604.018 143.549 607.768 Q143.549 611.494 145.378 613.67 Q147.23 615.823 150.378 615.823 Q153.526 615.823 155.355 613.67 Q157.207 611.494 157.207 607.768 Q157.207 604.018 155.355 601.865 Q153.526 599.712 150.378 599.712 M159.66 585.059 L159.66 589.319 Q157.901 588.485 156.096 588.045 Q154.313 587.606 152.554 587.606 Q147.924 587.606 145.471 590.731 Q143.04 593.856 142.693 600.175 Q144.059 598.161 146.119 597.096 Q148.179 596.008 150.656 596.008 Q155.864 596.008 158.873 599.18 Q161.906 602.328 161.906 607.768 Q161.906 613.092 158.758 616.309 Q155.61 619.527 150.378 619.527 Q144.383 619.527 141.211 614.943 Q138.04 610.337 138.04 601.61 Q138.04 593.416 141.929 588.555 Q145.818 583.67 152.369 583.67 Q154.128 583.67 155.91 584.018 Q157.716 584.365 159.66 585.059 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M139.684 423.292 L161.906 423.292 L161.906 425.283 L149.36 457.852 L144.475 457.852 L156.281 427.227 L139.684 427.227 L139.684 423.292 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M150.054 280.436 Q146.721 280.436 144.799 282.219 Q142.901 284.001 142.901 287.126 Q142.901 290.251 144.799 292.034 Q146.721 293.816 150.054 293.816 Q153.387 293.816 155.309 292.034 Q157.23 290.228 157.23 287.126 Q157.23 284.001 155.309 282.219 Q153.41 280.436 150.054 280.436 M145.378 278.446 Q142.369 277.705 140.679 275.645 Q139.012 273.585 139.012 270.622 Q139.012 266.478 141.952 264.071 Q144.915 261.663 150.054 261.663 Q155.216 261.663 158.156 264.071 Q161.096 266.478 161.096 270.622 Q161.096 273.585 159.406 275.645 Q157.739 277.705 154.753 278.446 Q158.133 279.233 160.008 281.524 Q161.906 283.816 161.906 287.126 Q161.906 292.149 158.827 294.834 Q155.772 297.52 150.054 297.52 Q144.336 297.52 141.258 294.834 Q138.202 292.149 138.202 287.126 Q138.202 283.816 140.1 281.524 Q141.998 279.233 145.378 278.446 M143.665 271.061 Q143.665 273.747 145.332 275.251 Q147.022 276.756 150.054 276.756 Q153.063 276.756 154.753 275.251 Q156.466 273.747 156.466 271.061 Q156.466 268.376 154.753 266.872 Q153.063 265.367 150.054 265.367 Q147.022 265.367 145.332 266.872 Q143.665 268.376 143.665 271.061 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M140.286 135.127 L140.286 130.868 Q142.045 131.701 143.85 132.141 Q145.656 132.581 147.392 132.581 Q152.022 132.581 154.452 129.479 Q156.906 126.354 157.253 120.012 Q155.91 122.002 153.85 123.067 Q151.79 124.132 149.29 124.132 Q144.105 124.132 141.073 121.007 Q138.063 117.859 138.063 112.419 Q138.063 107.095 141.211 103.877 Q144.36 100.66 149.591 100.66 Q155.586 100.66 158.734 105.266 Q161.906 109.85 161.906 118.6 Q161.906 126.771 158.017 131.655 Q154.151 136.516 147.6 136.516 Q145.841 136.516 144.035 136.169 Q142.23 135.822 140.286 135.127 M149.591 120.475 Q152.739 120.475 154.568 118.322 Q156.42 116.169 156.42 112.419 Q156.42 108.692 154.568 106.539 Q152.739 104.364 149.591 104.364 Q146.443 104.364 144.591 106.539 Q142.762 108.692 142.762 112.419 Q142.762 116.169 144.591 118.322 Q146.443 120.475 149.591 120.475 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M85.0042 766.698 Q85.7044 764.629 87.9961 762.688 Q90.2877 760.715 94.2981 758.741 L107.284 752.216 L107.284 759.123 L95.0938 765.202 Q90.3195 767.558 88.76 769.786 Q87.2004 771.982 87.2004 775.801 L87.2004 782.804 L107.284 782.804 L107.284 789.233 L59.7642 789.233 L59.7642 774.719 Q59.7642 766.571 63.1698 762.561 Q66.5755 758.55 73.4504 758.55 Q77.9382 758.55 80.8983 760.651 Q83.8584 762.72 85.0042 766.698 M65.0477 782.804 L81.9168 782.804 L81.9168 774.719 Q81.9168 770.072 79.7843 767.717 Q77.62 765.33 73.4504 765.33 Q69.2809 765.33 67.1802 767.717 Q65.0477 770.072 65.0477 774.719 L65.0477 782.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M75.7421 733.31 Q75.7421 738.021 79.4342 740.758 Q83.0945 743.495 89.492 743.495 Q95.8895 743.495 99.5817 740.79 Q103.242 738.053 103.242 733.31 Q103.242 728.631 99.5498 725.894 Q95.8577 723.157 89.492 723.157 Q83.1581 723.157 79.466 725.894 Q75.7421 728.631 75.7421 733.31 M70.7768 733.31 Q70.7768 725.671 75.7421 721.311 Q80.7073 716.95 89.492 716.95 Q98.2449 716.95 103.242 721.311 Q108.207 725.671 108.207 733.31 Q108.207 740.981 103.242 745.341 Q98.2449 749.67 89.492 749.67 Q80.7073 749.67 75.7421 745.341 Q70.7768 740.981 70.7768 733.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M75.7421 693.429 Q75.7421 698.14 79.4342 700.877 Q83.0945 703.614 89.492 703.614 Q95.8895 703.614 99.5817 700.909 Q103.242 698.171 103.242 693.429 Q103.242 688.75 99.5498 686.013 Q95.8577 683.276 89.492 683.276 Q83.1581 683.276 79.466 686.013 Q75.7421 688.75 75.7421 693.429 M70.7768 693.429 Q70.7768 685.79 75.7421 681.43 Q80.7073 677.069 89.492 677.069 Q98.2449 677.069 103.242 681.43 Q108.207 685.79 108.207 693.429 Q108.207 701.1 103.242 705.46 Q98.2449 709.789 89.492 709.789 Q80.7073 709.789 75.7421 705.46 Q70.7768 701.1 70.7768 693.429 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M61.5147 661.569 L71.6362 661.569 L71.6362 649.506 L76.1877 649.506 L76.1877 661.569 L95.5394 661.569 Q99.8999 661.569 101.141 660.391 Q102.383 659.182 102.383 655.521 L102.383 649.506 L107.284 649.506 L107.284 655.521 Q107.284 662.301 104.77 664.879 Q102.223 667.457 95.5394 667.457 L76.1877 667.457 L76.1877 671.754 L71.6362 671.754 L71.6362 667.457 L61.5147 667.457 L61.5147 661.569 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M59.7642 641.548 L59.7642 631.968 L92.1019 619.841 L59.7642 607.651 L59.7642 598.071 L107.284 598.071 L107.284 604.341 L65.557 604.341 L98.1494 616.595 L98.1494 623.056 L65.557 635.31 L107.284 635.31 L107.284 641.548 L59.7642 641.548 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M87.9961 555.07 L90.8606 555.07 L90.8606 581.997 Q96.9081 581.615 100.091 578.369 Q103.242 575.091 103.242 569.266 Q103.242 565.892 102.414 562.741 Q101.587 559.558 99.9318 556.439 L105.47 556.439 Q106.807 559.59 107.507 562.9 Q108.207 566.21 108.207 569.616 Q108.207 578.146 103.242 583.143 Q98.2767 588.108 89.8103 588.108 Q81.0574 588.108 75.9331 583.398 Q70.7768 578.655 70.7768 570.635 Q70.7768 563.441 75.4238 559.272 Q80.0389 555.07 87.9961 555.07 M86.2773 560.927 Q81.4712 560.99 78.6066 563.632 Q75.7421 566.242 75.7421 570.571 Q75.7421 575.472 78.5112 578.433 Q81.2802 581.361 86.3092 581.806 L86.2773 560.927 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M89.3647 529.257 Q89.3647 536.355 90.9879 539.092 Q92.6112 541.83 96.5261 541.83 Q99.6453 541.83 101.491 539.793 Q103.306 537.724 103.306 534.191 Q103.306 529.321 99.8681 526.393 Q96.3988 523.433 90.6697 523.433 L89.3647 523.433 L89.3647 529.257 M86.9457 517.576 L107.284 517.576 L107.284 523.433 L101.873 523.433 Q105.12 525.438 106.679 528.43 Q108.207 531.422 108.207 535.75 Q108.207 541.225 105.152 544.471 Q102.064 547.686 96.9081 547.686 Q90.8925 547.686 87.8369 543.676 Q84.7814 539.634 84.7814 531.645 L84.7814 523.433 L84.2085 523.433 Q80.1662 523.433 77.9701 526.106 Q75.7421 528.748 75.7421 533.554 Q75.7421 536.61 76.4741 539.506 Q77.2062 542.403 78.6703 545.076 L73.2595 545.076 Q72.0181 541.862 71.4134 538.838 Q70.7768 535.814 70.7768 532.95 Q70.7768 525.215 74.7872 521.396 Q78.7976 517.576 86.9457 517.576 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M85.7681 475.881 L107.284 475.881 L107.284 481.737 L85.959 481.737 Q80.8983 481.737 78.3838 483.711 Q75.8694 485.684 75.8694 489.631 Q75.8694 494.373 78.8931 497.111 Q81.9168 499.848 87.1367 499.848 L107.284 499.848 L107.284 505.736 L71.6362 505.736 L71.6362 499.848 L77.1744 499.848 Q73.9597 497.747 72.3683 494.914 Q70.7768 492.05 70.7768 488.326 Q70.7768 482.183 74.5963 479.032 Q78.3838 475.881 85.7681 475.881 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M61.3238 435.459 L67.594 435.459 Q65.8434 439.119 64.984 442.366 Q64.1247 445.612 64.1247 448.636 Q64.1247 453.887 66.1617 456.752 Q68.1987 459.585 71.9545 459.585 Q75.1055 459.585 76.7288 457.707 Q78.3202 455.797 79.3069 450.514 L80.1026 446.631 Q81.4712 439.437 84.9405 436.032 Q88.378 432.594 94.1708 432.594 Q101.078 432.594 104.642 437.241 Q108.207 441.856 108.207 450.8 Q108.207 454.174 107.443 457.993 Q106.679 461.781 105.183 465.855 L98.5631 465.855 Q100.759 461.94 101.873 458.184 Q102.987 454.429 102.987 450.8 Q102.987 445.294 100.823 442.302 Q98.6586 439.31 94.6482 439.31 Q91.1471 439.31 89.1737 441.474 Q87.2004 443.607 86.2137 448.508 L85.4498 452.423 Q84.0175 459.617 80.962 462.831 Q77.9064 466.046 72.4637 466.046 Q66.1617 466.046 62.5332 461.622 Q58.9048 457.166 58.9048 449.368 Q58.9048 446.026 59.5095 442.557 Q60.1143 439.087 61.3238 435.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M89.492 419.322 Q95.9532 419.322 99.6453 416.68 Q103.306 414.006 103.306 409.359 Q103.306 404.712 99.6453 402.039 Q95.9532 399.365 89.492 399.365 Q83.0308 399.365 79.3705 402.039 Q75.6784 404.712 75.6784 409.359 Q75.6784 414.006 79.3705 416.68 Q83.0308 419.322 89.492 419.322 M101.937 399.365 Q105.12 401.211 106.679 404.044 Q108.207 406.845 108.207 410.792 Q108.207 417.253 103.051 421.327 Q97.8947 425.369 89.492 425.369 Q81.0893 425.369 75.9331 421.327 Q70.7768 417.253 70.7768 410.792 Q70.7768 406.845 72.3364 404.044 Q73.8642 401.211 77.0471 399.365 L71.6362 399.365 L71.6362 393.509 L120.843 393.509 L120.843 399.365 L101.937 399.365 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M93.2159 382.05 L71.6362 382.05 L71.6362 376.194 L92.9931 376.194 Q98.0539 376.194 100.6 374.221 Q103.115 372.247 103.115 368.301 Q103.115 363.558 100.091 360.821 Q97.0672 358.052 91.8473 358.052 L71.6362 358.052 L71.6362 352.195 L107.284 352.195 L107.284 358.052 L101.81 358.052 Q105.056 360.184 106.648 363.017 Q108.207 365.818 108.207 369.542 Q108.207 375.685 104.388 378.868 Q100.568 382.05 93.2159 382.05 M70.7768 367.314 L70.7768 367.314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M89.3647 323.932 Q89.3647 331.029 90.9879 333.767 Q92.6112 336.504 96.5261 336.504 Q99.6453 336.504 101.491 334.467 Q103.306 332.398 103.306 328.865 Q103.306 323.995 99.8681 321.067 Q96.3988 318.107 90.6697 318.107 L89.3647 318.107 L89.3647 323.932 M86.9457 312.25 L107.284 312.25 L107.284 318.107 L101.873 318.107 Q105.12 320.112 106.679 323.104 Q108.207 326.096 108.207 330.425 Q108.207 335.899 105.152 339.146 Q102.064 342.36 96.9081 342.36 Q90.8925 342.36 87.8369 338.35 Q84.7814 334.308 84.7814 326.319 L84.7814 318.107 L84.2085 318.107 Q80.1662 318.107 77.9701 320.781 Q75.7421 323.422 75.7421 328.228 Q75.7421 331.284 76.4741 334.18 Q77.2062 337.077 78.6703 339.75 L73.2595 339.75 Q72.0181 336.536 71.4134 333.512 Q70.7768 330.488 70.7768 327.624 Q70.7768 319.889 74.7872 316.07 Q78.7976 312.25 86.9457 312.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M77.1107 279.531 Q76.5378 280.517 76.2832 281.695 Q75.9967 282.841 75.9967 284.241 Q75.9967 289.207 79.2432 291.88 Q82.4579 294.522 88.5053 294.522 L107.284 294.522 L107.284 300.41 L71.6362 300.41 L71.6362 294.522 L77.1744 294.522 Q73.9279 292.676 72.3683 289.716 Q70.7768 286.756 70.7768 282.523 Q70.7768 281.918 70.8723 281.186 Q70.936 280.454 71.0951 279.563 L77.1107 279.531 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M87.9961 244.328 L90.8606 244.328 L90.8606 271.255 Q96.9081 270.873 100.091 267.627 Q103.242 264.349 103.242 258.524 Q103.242 255.15 102.414 251.999 Q101.587 248.816 99.9318 245.697 L105.47 245.697 Q106.807 248.848 107.507 252.158 Q108.207 255.468 108.207 258.874 Q108.207 267.404 103.242 272.401 Q98.2767 277.366 89.8103 277.366 Q81.0574 277.366 75.9331 272.656 Q70.7768 267.913 70.7768 259.893 Q70.7768 252.699 75.4238 248.53 Q80.0389 244.328 87.9961 244.328 M86.2773 250.185 Q81.4712 250.249 78.6066 252.89 Q75.7421 255.5 75.7421 259.829 Q75.7421 264.73 78.5112 267.691 Q81.2802 270.619 86.3092 271.064 L86.2773 250.185 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M77.0471 211.259 L57.759 211.259 L57.759 205.402 L107.284 205.402 L107.284 211.259 L101.937 211.259 Q105.12 213.105 106.679 215.937 Q108.207 218.738 108.207 222.685 Q108.207 229.146 103.051 233.22 Q97.8947 237.262 89.492 237.262 Q81.0893 237.262 75.9331 233.22 Q70.7768 229.146 70.7768 222.685 Q70.7768 218.738 72.3364 215.937 Q73.8642 213.105 77.0471 211.259 M89.492 231.215 Q95.9532 231.215 99.6453 228.573 Q103.306 225.9 103.306 221.253 Q103.306 216.606 99.6453 213.932 Q95.9532 211.259 89.492 211.259 Q83.0308 211.259 79.3705 213.932 Q75.6784 216.606 75.6784 221.253 Q75.6784 225.9 79.3705 228.573 Q83.0308 231.215 89.492 231.215 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M59.7642 193.084 L59.7642 163.038 L65.175 163.038 L65.175 186.655 L79.2432 186.655 L79.2432 164.025 L84.6541 164.025 L84.6541 186.655 L101.873 186.655 L101.873 162.465 L107.284 162.465 L107.284 193.084 L59.7642 193.084 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M77.1107 131.496 Q76.5378 132.483 76.2832 133.661 Q75.9967 134.806 75.9967 136.207 Q75.9967 141.172 79.2432 143.846 Q82.4579 146.487 88.5053 146.487 L107.284 146.487 L107.284 152.376 L71.6362 152.376 L71.6362 146.487 L77.1744 146.487 Q73.9279 144.641 72.3683 141.681 Q70.7768 138.721 70.7768 134.488 Q70.7768 133.883 70.8723 133.151 Q70.936 132.419 71.0951 131.528 L77.1107 131.496 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M77.1107 105.842 Q76.5378 106.829 76.2832 108.007 Q75.9967 109.153 75.9967 110.553 Q75.9967 115.518 79.2432 118.192 Q82.4579 120.834 88.5053 120.834 L107.284 120.834 L107.284 126.722 L71.6362 126.722 L71.6362 120.834 L77.1744 120.834 Q73.9279 118.988 72.3683 116.028 Q70.7768 113.068 70.7768 108.834 Q70.7768 108.23 70.8723 107.498 Q70.936 106.765 71.0951 105.874 L77.1107 105.842 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M75.7421 87.3182 Q75.7421 92.0289 79.4342 94.7661 Q83.0945 97.5034 89.492 97.5034 Q95.8895 97.5034 99.5817 94.798 Q103.242 92.0607 103.242 87.3182 Q103.242 82.6395 99.5498 79.9022 Q95.8577 77.1649 89.492 77.1649 Q83.1581 77.1649 79.466 79.9022 Q75.7421 82.6395 75.7421 87.3182 M70.7768 87.3182 Q70.7768 79.6794 75.7421 75.3189 Q80.7073 70.9584 89.492 70.9584 Q98.2449 70.9584 103.242 75.3189 Q108.207 79.6794 108.207 87.3182 Q108.207 94.9889 103.242 99.3494 Q98.2449 103.678 89.492 103.678 Q80.7073 103.678 75.7421 99.3494 Q70.7768 94.9889 70.7768 87.3182 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip150)\" d=\"M77.1107 40.594 Q76.5378 41.5806 76.2832 42.7583 Q75.9967 43.9041 75.9967 45.3046 Q75.9967 50.2698 79.2432 52.9434 Q82.4579 55.5852 88.5053 55.5852 L107.284 55.5852 L107.284 61.4735 L71.6362 61.4735 L71.6362 55.5852 L77.1744 55.5852 Q73.9279 53.7391 72.3683 50.7791 Q70.7768 47.819 70.7768 43.5858 Q70.7768 42.9811 70.8723 42.249 Q70.936 41.517 71.0951 40.6258 L77.1107 40.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip152)\" cx=\"478.104\" cy=\"761.771\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"386.453\" cy=\"761.562\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"844.708\" cy=\"653.253\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"1028.01\" cy=\"68.0556\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"936.359\" cy=\"434.508\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"753.057\" cy=\"702.144\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"661.406\" cy=\"742.356\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"569.755\" cy=\"760.816\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"294.802\" cy=\"761.125\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n<circle clip-path=\"url(#clip152)\" cx=\"203.152\" cy=\"761.021\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n</svg>\n",
      "text/html": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"275\" height=\"250\" viewBox=\"0 0 1100 1000\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"1100\" height=\"1000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip200)\" d=\"\n",
       "M0 1000 L1100 1000 L1100 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip201\">\n",
       "    <rect x=\"220\" y=\"10\" width=\"771\" height=\"771\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip200)\" d=\"\n",
       "M178.406 782.583 L1052.76 782.583 L1052.76 47.2441 L178.406 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip202\">\n",
       "    <rect x=\"178\" y=\"47\" width=\"875\" height=\"736\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  409.366,782.583 409.366,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  821.795,782.583 821.795,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,782.583 1052.76,782.583 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,47.2441 1052.76,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  409.366,782.583 409.366,763.685 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  821.795,782.583 821.795,763.685 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip200)\" d=\"M359.632 846.308 L367.271 846.308 L367.271 819.942 L358.961 821.609 L358.961 817.349 L367.224 815.683 L371.9 815.683 L371.9 846.308 L379.539 846.308 L379.539 850.243 L359.632 850.243 L359.632 846.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M398.984 818.761 Q395.372 818.761 393.544 822.326 Q391.738 825.868 391.738 832.997 Q391.738 840.104 393.544 843.669 Q395.372 847.21 398.984 847.21 Q402.618 847.21 404.423 843.669 Q406.252 840.104 406.252 832.997 Q406.252 825.868 404.423 822.326 Q402.618 818.761 398.984 818.761 M398.984 815.058 Q404.794 815.058 407.849 819.664 Q410.928 824.248 410.928 832.997 Q410.928 841.724 407.849 846.331 Q404.794 850.914 398.984 850.914 Q393.173 850.914 390.095 846.331 Q387.039 841.724 387.039 832.997 Q387.039 824.248 390.095 819.664 Q393.173 815.058 398.984 815.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M410.928 809.159 L435.04 809.159 L435.04 812.356 L410.928 812.356 L410.928 809.159 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M446.512 819.635 L459.772 819.635 L459.772 822.832 L441.942 822.832 L441.942 819.635 Q444.105 817.397 447.829 813.635 Q451.572 809.855 452.531 808.764 Q454.355 806.714 455.07 805.304 Q455.803 803.874 455.803 802.501 Q455.803 800.263 454.223 798.852 Q452.662 797.442 450.142 797.442 Q448.355 797.442 446.362 798.063 Q444.387 798.683 442.13 799.943 L442.13 796.107 Q444.425 795.185 446.418 794.715 Q448.412 794.245 450.067 794.245 Q454.43 794.245 457.026 796.426 Q459.621 798.608 459.621 802.257 Q459.621 803.987 458.963 805.548 Q458.324 807.09 456.612 809.197 Q456.142 809.742 453.622 812.356 Q451.101 814.952 446.512 819.635 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M786.778 846.308 L794.417 846.308 L794.417 819.942 L786.107 821.609 L786.107 817.349 L794.371 815.683 L799.047 815.683 L799.047 846.308 L806.686 846.308 L806.686 850.243 L786.778 850.243 L786.778 846.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M826.13 818.761 Q822.519 818.761 820.69 822.326 Q818.885 825.868 818.885 832.997 Q818.885 840.104 820.69 843.669 Q822.519 847.21 826.13 847.21 Q829.764 847.21 831.57 843.669 Q833.398 840.104 833.398 832.997 Q833.398 825.868 831.57 822.326 Q829.764 818.761 826.13 818.761 M826.13 815.058 Q831.94 815.058 834.996 819.664 Q838.074 824.248 838.074 832.997 Q838.074 841.724 834.996 846.331 Q831.94 850.914 826.13 850.914 Q820.32 850.914 817.241 846.331 Q814.186 841.724 814.186 832.997 Q814.186 824.248 817.241 819.664 Q820.32 815.058 826.13 815.058 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M847.779 797.254 Q844.845 797.254 843.359 800.15 Q841.892 803.028 841.892 808.821 Q841.892 814.595 843.359 817.491 Q844.845 820.369 847.779 820.369 Q850.732 820.369 852.199 817.491 Q853.685 814.595 853.685 808.821 Q853.685 803.028 852.199 800.15 Q850.732 797.254 847.779 797.254 M847.779 794.245 Q852.5 794.245 854.983 797.987 Q857.484 801.711 857.484 808.821 Q857.484 815.911 854.983 819.654 Q852.5 823.378 847.779 823.378 Q843.058 823.378 840.557 819.654 Q838.074 815.911 838.074 808.821 Q838.074 801.711 840.557 797.987 Q843.058 794.245 847.779 794.245 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M499.486 876.846 L505.343 876.846 L505.343 926.372 L499.486 926.372 L499.486 876.846 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M533.797 908.452 Q526.7 908.452 523.962 910.075 Q521.225 911.699 521.225 915.614 Q521.225 918.733 523.262 920.579 Q525.331 922.393 528.864 922.393 Q533.734 922.393 536.662 918.956 Q539.622 915.486 539.622 909.757 L539.622 908.452 L533.797 908.452 M545.478 906.033 L545.478 926.372 L539.622 926.372 L539.622 920.961 Q537.617 924.207 534.625 925.767 Q531.633 927.295 527.304 927.295 Q521.83 927.295 518.583 924.239 Q515.369 921.152 515.369 915.995 Q515.369 909.98 519.379 906.924 Q523.421 903.869 531.41 903.869 L539.622 903.869 L539.622 903.296 Q539.622 899.254 536.948 897.057 Q534.307 894.829 529.501 894.829 Q526.445 894.829 523.549 895.562 Q520.652 896.294 517.979 897.758 L517.979 892.347 Q521.193 891.106 524.217 890.501 Q527.241 889.864 530.105 889.864 Q537.84 889.864 541.659 893.875 Q545.478 897.885 545.478 906.033 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M585.296 897.567 Q587.492 893.62 590.548 891.742 Q593.603 889.864 597.741 889.864 Q603.311 889.864 606.335 893.779 Q609.358 897.662 609.358 904.855 L609.358 926.372 L603.47 926.372 L603.47 905.046 Q603.47 899.922 601.656 897.439 Q599.842 894.957 596.118 894.957 Q591.566 894.957 588.924 897.981 Q586.283 901.004 586.283 906.224 L586.283 926.372 L580.394 926.372 L580.394 905.046 Q580.394 899.89 578.58 897.439 Q576.766 894.957 572.978 894.957 Q568.491 894.957 565.849 898.012 Q563.207 901.036 563.207 906.224 L563.207 926.372 L557.319 926.372 L557.319 890.724 L563.207 890.724 L563.207 896.262 Q565.212 892.983 568.013 891.424 Q570.814 889.864 574.665 889.864 Q578.548 889.864 581.254 891.838 Q583.991 893.811 585.296 897.567 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M646.63 908.579 Q646.63 902.118 643.956 898.458 Q641.314 894.766 636.667 894.766 Q632.02 894.766 629.347 898.458 Q626.705 902.118 626.705 908.579 Q626.705 915.041 629.347 918.733 Q632.02 922.393 636.667 922.393 Q641.314 922.393 643.956 918.733 Q646.63 915.041 646.63 908.579 M626.705 896.134 Q628.551 892.952 631.352 891.424 Q634.185 889.864 638.1 889.864 Q644.593 889.864 648.635 895.02 Q652.709 900.177 652.709 908.579 Q652.709 916.982 648.635 922.138 Q644.593 927.295 638.1 927.295 Q634.185 927.295 631.352 925.767 Q628.551 924.207 626.705 921.024 L626.705 926.372 L620.817 926.372 L620.817 876.846 L626.705 876.846 L626.705 896.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M685.874 896.134 L685.874 876.846 L691.731 876.846 L691.731 926.372 L685.874 926.372 L685.874 921.024 Q684.028 924.207 681.195 925.767 Q678.394 927.295 674.448 927.295 Q667.987 927.295 663.912 922.138 Q659.87 916.982 659.87 908.579 Q659.87 900.177 663.912 895.02 Q667.987 889.864 674.448 889.864 Q678.394 889.864 681.195 891.424 Q684.028 892.952 685.874 896.134 M665.918 908.579 Q665.918 915.041 668.559 918.733 Q671.233 922.393 675.88 922.393 Q680.527 922.393 683.201 918.733 Q685.874 915.041 685.874 908.579 Q685.874 902.118 683.201 898.458 Q680.527 894.766 675.88 894.766 Q671.233 894.766 668.559 898.458 Q665.918 902.118 665.918 908.579 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M719.994 908.452 Q712.897 908.452 710.159 910.075 Q707.422 911.699 707.422 915.614 Q707.422 918.733 709.459 920.579 Q711.528 922.393 715.061 922.393 Q719.931 922.393 722.859 918.956 Q725.819 915.486 725.819 909.757 L725.819 908.452 L719.994 908.452 M731.675 906.033 L731.675 926.372 L725.819 926.372 L725.819 920.961 Q723.814 924.207 720.822 925.767 Q717.83 927.295 713.501 927.295 Q708.027 927.295 704.78 924.239 Q701.566 921.152 701.566 915.995 Q701.566 909.98 705.576 906.924 Q709.618 903.869 717.607 903.869 L725.819 903.869 L725.819 903.296 Q725.819 899.254 723.145 897.057 Q720.504 894.829 715.697 894.829 Q712.642 894.829 709.746 895.562 Q706.849 896.294 704.176 897.758 L704.176 892.347 Q707.39 891.106 710.414 890.501 Q713.438 889.864 716.302 889.864 Q724.037 889.864 727.856 893.875 Q731.675 897.885 731.675 906.033 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  178.406,762.579 1052.76,762.579 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  178.406,601.575 1052.76,601.575 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  178.406,440.572 1052.76,440.572 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  178.406,279.568 1052.76,279.568 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  178.406,118.565 1052.76,118.565 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,782.583 178.406,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1052.76,782.583 1052.76,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,762.579 197.303,762.579 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,601.575 197.303,601.575 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,440.572 197.303,440.572 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,279.568 197.303,279.568 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  178.406,118.565 197.303,118.565 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip200)\" d=\"M141.003 745.299 L159.359 745.299 L159.359 749.234 L145.285 749.234 L145.285 757.706 Q146.304 757.359 147.323 757.197 Q148.341 757.012 149.36 757.012 Q155.147 757.012 158.526 760.183 Q161.906 763.354 161.906 768.771 Q161.906 774.35 158.434 777.452 Q154.961 780.53 148.642 780.53 Q146.466 780.53 144.198 780.16 Q141.952 779.789 139.545 779.049 L139.545 774.35 Q141.628 775.484 143.85 776.04 Q146.073 776.595 148.549 776.595 Q152.554 776.595 154.892 774.489 Q157.23 772.382 157.23 768.771 Q157.23 765.16 154.892 763.053 Q152.554 760.947 148.549 760.947 Q146.674 760.947 144.799 761.364 Q142.948 761.78 141.003 762.66 L141.003 745.299 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M150.378 599.712 Q147.23 599.712 145.378 601.865 Q143.549 604.018 143.549 607.768 Q143.549 611.494 145.378 613.67 Q147.23 615.823 150.378 615.823 Q153.526 615.823 155.355 613.67 Q157.207 611.494 157.207 607.768 Q157.207 604.018 155.355 601.865 Q153.526 599.712 150.378 599.712 M159.66 585.059 L159.66 589.319 Q157.901 588.485 156.096 588.045 Q154.313 587.606 152.554 587.606 Q147.924 587.606 145.471 590.731 Q143.04 593.856 142.693 600.175 Q144.059 598.161 146.119 597.096 Q148.179 596.008 150.656 596.008 Q155.864 596.008 158.873 599.18 Q161.906 602.328 161.906 607.768 Q161.906 613.092 158.758 616.309 Q155.61 619.527 150.378 619.527 Q144.383 619.527 141.211 614.943 Q138.04 610.337 138.04 601.61 Q138.04 593.416 141.929 588.555 Q145.818 583.67 152.369 583.67 Q154.128 583.67 155.91 584.018 Q157.716 584.365 159.66 585.059 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M139.684 423.292 L161.906 423.292 L161.906 425.283 L149.36 457.852 L144.475 457.852 L156.281 427.227 L139.684 427.227 L139.684 423.292 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M150.054 280.436 Q146.721 280.436 144.799 282.219 Q142.901 284.001 142.901 287.126 Q142.901 290.251 144.799 292.034 Q146.721 293.816 150.054 293.816 Q153.387 293.816 155.309 292.034 Q157.23 290.228 157.23 287.126 Q157.23 284.001 155.309 282.219 Q153.41 280.436 150.054 280.436 M145.378 278.446 Q142.369 277.705 140.679 275.645 Q139.012 273.585 139.012 270.622 Q139.012 266.478 141.952 264.071 Q144.915 261.663 150.054 261.663 Q155.216 261.663 158.156 264.071 Q161.096 266.478 161.096 270.622 Q161.096 273.585 159.406 275.645 Q157.739 277.705 154.753 278.446 Q158.133 279.233 160.008 281.524 Q161.906 283.816 161.906 287.126 Q161.906 292.149 158.827 294.834 Q155.772 297.52 150.054 297.52 Q144.336 297.52 141.258 294.834 Q138.202 292.149 138.202 287.126 Q138.202 283.816 140.1 281.524 Q141.998 279.233 145.378 278.446 M143.665 271.061 Q143.665 273.747 145.332 275.251 Q147.022 276.756 150.054 276.756 Q153.063 276.756 154.753 275.251 Q156.466 273.747 156.466 271.061 Q156.466 268.376 154.753 266.872 Q153.063 265.367 150.054 265.367 Q147.022 265.367 145.332 266.872 Q143.665 268.376 143.665 271.061 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M140.286 135.127 L140.286 130.868 Q142.045 131.701 143.85 132.141 Q145.656 132.581 147.392 132.581 Q152.022 132.581 154.452 129.479 Q156.906 126.354 157.253 120.012 Q155.91 122.002 153.85 123.067 Q151.79 124.132 149.29 124.132 Q144.105 124.132 141.073 121.007 Q138.063 117.859 138.063 112.419 Q138.063 107.095 141.211 103.877 Q144.36 100.66 149.591 100.66 Q155.586 100.66 158.734 105.266 Q161.906 109.85 161.906 118.6 Q161.906 126.771 158.017 131.655 Q154.151 136.516 147.6 136.516 Q145.841 136.516 144.035 136.169 Q142.23 135.822 140.286 135.127 M149.591 120.475 Q152.739 120.475 154.568 118.322 Q156.42 116.169 156.42 112.419 Q156.42 108.692 154.568 106.539 Q152.739 104.364 149.591 104.364 Q146.443 104.364 144.591 106.539 Q142.762 108.692 142.762 112.419 Q142.762 116.169 144.591 118.322 Q146.443 120.475 149.591 120.475 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M85.0042 766.698 Q85.7044 764.629 87.9961 762.688 Q90.2877 760.715 94.2981 758.741 L107.284 752.216 L107.284 759.123 L95.0938 765.202 Q90.3195 767.558 88.76 769.786 Q87.2004 771.982 87.2004 775.801 L87.2004 782.804 L107.284 782.804 L107.284 789.233 L59.7642 789.233 L59.7642 774.719 Q59.7642 766.571 63.1698 762.561 Q66.5755 758.55 73.4504 758.55 Q77.9382 758.55 80.8983 760.651 Q83.8584 762.72 85.0042 766.698 M65.0477 782.804 L81.9168 782.804 L81.9168 774.719 Q81.9168 770.072 79.7843 767.717 Q77.62 765.33 73.4504 765.33 Q69.2809 765.33 67.1802 767.717 Q65.0477 770.072 65.0477 774.719 L65.0477 782.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M75.7421 733.31 Q75.7421 738.021 79.4342 740.758 Q83.0945 743.495 89.492 743.495 Q95.8895 743.495 99.5817 740.79 Q103.242 738.053 103.242 733.31 Q103.242 728.631 99.5498 725.894 Q95.8577 723.157 89.492 723.157 Q83.1581 723.157 79.466 725.894 Q75.7421 728.631 75.7421 733.31 M70.7768 733.31 Q70.7768 725.671 75.7421 721.311 Q80.7073 716.95 89.492 716.95 Q98.2449 716.95 103.242 721.311 Q108.207 725.671 108.207 733.31 Q108.207 740.981 103.242 745.341 Q98.2449 749.67 89.492 749.67 Q80.7073 749.67 75.7421 745.341 Q70.7768 740.981 70.7768 733.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M75.7421 693.429 Q75.7421 698.14 79.4342 700.877 Q83.0945 703.614 89.492 703.614 Q95.8895 703.614 99.5817 700.909 Q103.242 698.171 103.242 693.429 Q103.242 688.75 99.5498 686.013 Q95.8577 683.276 89.492 683.276 Q83.1581 683.276 79.466 686.013 Q75.7421 688.75 75.7421 693.429 M70.7768 693.429 Q70.7768 685.79 75.7421 681.43 Q80.7073 677.069 89.492 677.069 Q98.2449 677.069 103.242 681.43 Q108.207 685.79 108.207 693.429 Q108.207 701.1 103.242 705.46 Q98.2449 709.789 89.492 709.789 Q80.7073 709.789 75.7421 705.46 Q70.7768 701.1 70.7768 693.429 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M61.5147 661.569 L71.6362 661.569 L71.6362 649.506 L76.1877 649.506 L76.1877 661.569 L95.5394 661.569 Q99.8999 661.569 101.141 660.391 Q102.383 659.182 102.383 655.521 L102.383 649.506 L107.284 649.506 L107.284 655.521 Q107.284 662.301 104.77 664.879 Q102.223 667.457 95.5394 667.457 L76.1877 667.457 L76.1877 671.754 L71.6362 671.754 L71.6362 667.457 L61.5147 667.457 L61.5147 661.569 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M59.7642 641.548 L59.7642 631.968 L92.1019 619.841 L59.7642 607.651 L59.7642 598.071 L107.284 598.071 L107.284 604.341 L65.557 604.341 L98.1494 616.595 L98.1494 623.056 L65.557 635.31 L107.284 635.31 L107.284 641.548 L59.7642 641.548 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M87.9961 555.07 L90.8606 555.07 L90.8606 581.997 Q96.9081 581.615 100.091 578.369 Q103.242 575.091 103.242 569.266 Q103.242 565.892 102.414 562.741 Q101.587 559.558 99.9318 556.439 L105.47 556.439 Q106.807 559.59 107.507 562.9 Q108.207 566.21 108.207 569.616 Q108.207 578.146 103.242 583.143 Q98.2767 588.108 89.8103 588.108 Q81.0574 588.108 75.9331 583.398 Q70.7768 578.655 70.7768 570.635 Q70.7768 563.441 75.4238 559.272 Q80.0389 555.07 87.9961 555.07 M86.2773 560.927 Q81.4712 560.99 78.6066 563.632 Q75.7421 566.242 75.7421 570.571 Q75.7421 575.472 78.5112 578.433 Q81.2802 581.361 86.3092 581.806 L86.2773 560.927 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M89.3647 529.257 Q89.3647 536.355 90.9879 539.092 Q92.6112 541.83 96.5261 541.83 Q99.6453 541.83 101.491 539.793 Q103.306 537.724 103.306 534.191 Q103.306 529.321 99.8681 526.393 Q96.3988 523.433 90.6697 523.433 L89.3647 523.433 L89.3647 529.257 M86.9457 517.576 L107.284 517.576 L107.284 523.433 L101.873 523.433 Q105.12 525.438 106.679 528.43 Q108.207 531.422 108.207 535.75 Q108.207 541.225 105.152 544.471 Q102.064 547.686 96.9081 547.686 Q90.8925 547.686 87.8369 543.676 Q84.7814 539.634 84.7814 531.645 L84.7814 523.433 L84.2085 523.433 Q80.1662 523.433 77.9701 526.106 Q75.7421 528.748 75.7421 533.554 Q75.7421 536.61 76.4741 539.506 Q77.2062 542.403 78.6703 545.076 L73.2595 545.076 Q72.0181 541.862 71.4134 538.838 Q70.7768 535.814 70.7768 532.95 Q70.7768 525.215 74.7872 521.396 Q78.7976 517.576 86.9457 517.576 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M85.7681 475.881 L107.284 475.881 L107.284 481.737 L85.959 481.737 Q80.8983 481.737 78.3838 483.711 Q75.8694 485.684 75.8694 489.631 Q75.8694 494.373 78.8931 497.111 Q81.9168 499.848 87.1367 499.848 L107.284 499.848 L107.284 505.736 L71.6362 505.736 L71.6362 499.848 L77.1744 499.848 Q73.9597 497.747 72.3683 494.914 Q70.7768 492.05 70.7768 488.326 Q70.7768 482.183 74.5963 479.032 Q78.3838 475.881 85.7681 475.881 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M61.3238 435.459 L67.594 435.459 Q65.8434 439.119 64.984 442.366 Q64.1247 445.612 64.1247 448.636 Q64.1247 453.887 66.1617 456.752 Q68.1987 459.585 71.9545 459.585 Q75.1055 459.585 76.7288 457.707 Q78.3202 455.797 79.3069 450.514 L80.1026 446.631 Q81.4712 439.437 84.9405 436.032 Q88.378 432.594 94.1708 432.594 Q101.078 432.594 104.642 437.241 Q108.207 441.856 108.207 450.8 Q108.207 454.174 107.443 457.993 Q106.679 461.781 105.183 465.855 L98.5631 465.855 Q100.759 461.94 101.873 458.184 Q102.987 454.429 102.987 450.8 Q102.987 445.294 100.823 442.302 Q98.6586 439.31 94.6482 439.31 Q91.1471 439.31 89.1737 441.474 Q87.2004 443.607 86.2137 448.508 L85.4498 452.423 Q84.0175 459.617 80.962 462.831 Q77.9064 466.046 72.4637 466.046 Q66.1617 466.046 62.5332 461.622 Q58.9048 457.166 58.9048 449.368 Q58.9048 446.026 59.5095 442.557 Q60.1143 439.087 61.3238 435.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M89.492 419.322 Q95.9532 419.322 99.6453 416.68 Q103.306 414.006 103.306 409.359 Q103.306 404.712 99.6453 402.039 Q95.9532 399.365 89.492 399.365 Q83.0308 399.365 79.3705 402.039 Q75.6784 404.712 75.6784 409.359 Q75.6784 414.006 79.3705 416.68 Q83.0308 419.322 89.492 419.322 M101.937 399.365 Q105.12 401.211 106.679 404.044 Q108.207 406.845 108.207 410.792 Q108.207 417.253 103.051 421.327 Q97.8947 425.369 89.492 425.369 Q81.0893 425.369 75.9331 421.327 Q70.7768 417.253 70.7768 410.792 Q70.7768 406.845 72.3364 404.044 Q73.8642 401.211 77.0471 399.365 L71.6362 399.365 L71.6362 393.509 L120.843 393.509 L120.843 399.365 L101.937 399.365 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M93.2159 382.05 L71.6362 382.05 L71.6362 376.194 L92.9931 376.194 Q98.0539 376.194 100.6 374.221 Q103.115 372.247 103.115 368.301 Q103.115 363.558 100.091 360.821 Q97.0672 358.052 91.8473 358.052 L71.6362 358.052 L71.6362 352.195 L107.284 352.195 L107.284 358.052 L101.81 358.052 Q105.056 360.184 106.648 363.017 Q108.207 365.818 108.207 369.542 Q108.207 375.685 104.388 378.868 Q100.568 382.05 93.2159 382.05 M70.7768 367.314 L70.7768 367.314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M89.3647 323.932 Q89.3647 331.029 90.9879 333.767 Q92.6112 336.504 96.5261 336.504 Q99.6453 336.504 101.491 334.467 Q103.306 332.398 103.306 328.865 Q103.306 323.995 99.8681 321.067 Q96.3988 318.107 90.6697 318.107 L89.3647 318.107 L89.3647 323.932 M86.9457 312.25 L107.284 312.25 L107.284 318.107 L101.873 318.107 Q105.12 320.112 106.679 323.104 Q108.207 326.096 108.207 330.425 Q108.207 335.899 105.152 339.146 Q102.064 342.36 96.9081 342.36 Q90.8925 342.36 87.8369 338.35 Q84.7814 334.308 84.7814 326.319 L84.7814 318.107 L84.2085 318.107 Q80.1662 318.107 77.9701 320.781 Q75.7421 323.422 75.7421 328.228 Q75.7421 331.284 76.4741 334.18 Q77.2062 337.077 78.6703 339.75 L73.2595 339.75 Q72.0181 336.536 71.4134 333.512 Q70.7768 330.488 70.7768 327.624 Q70.7768 319.889 74.7872 316.07 Q78.7976 312.25 86.9457 312.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M77.1107 279.531 Q76.5378 280.517 76.2832 281.695 Q75.9967 282.841 75.9967 284.241 Q75.9967 289.207 79.2432 291.88 Q82.4579 294.522 88.5053 294.522 L107.284 294.522 L107.284 300.41 L71.6362 300.41 L71.6362 294.522 L77.1744 294.522 Q73.9279 292.676 72.3683 289.716 Q70.7768 286.756 70.7768 282.523 Q70.7768 281.918 70.8723 281.186 Q70.936 280.454 71.0951 279.563 L77.1107 279.531 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M87.9961 244.328 L90.8606 244.328 L90.8606 271.255 Q96.9081 270.873 100.091 267.627 Q103.242 264.349 103.242 258.524 Q103.242 255.15 102.414 251.999 Q101.587 248.816 99.9318 245.697 L105.47 245.697 Q106.807 248.848 107.507 252.158 Q108.207 255.468 108.207 258.874 Q108.207 267.404 103.242 272.401 Q98.2767 277.366 89.8103 277.366 Q81.0574 277.366 75.9331 272.656 Q70.7768 267.913 70.7768 259.893 Q70.7768 252.699 75.4238 248.53 Q80.0389 244.328 87.9961 244.328 M86.2773 250.185 Q81.4712 250.249 78.6066 252.89 Q75.7421 255.5 75.7421 259.829 Q75.7421 264.73 78.5112 267.691 Q81.2802 270.619 86.3092 271.064 L86.2773 250.185 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M77.0471 211.259 L57.759 211.259 L57.759 205.402 L107.284 205.402 L107.284 211.259 L101.937 211.259 Q105.12 213.105 106.679 215.937 Q108.207 218.738 108.207 222.685 Q108.207 229.146 103.051 233.22 Q97.8947 237.262 89.492 237.262 Q81.0893 237.262 75.9331 233.22 Q70.7768 229.146 70.7768 222.685 Q70.7768 218.738 72.3364 215.937 Q73.8642 213.105 77.0471 211.259 M89.492 231.215 Q95.9532 231.215 99.6453 228.573 Q103.306 225.9 103.306 221.253 Q103.306 216.606 99.6453 213.932 Q95.9532 211.259 89.492 211.259 Q83.0308 211.259 79.3705 213.932 Q75.6784 216.606 75.6784 221.253 Q75.6784 225.9 79.3705 228.573 Q83.0308 231.215 89.492 231.215 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M59.7642 193.084 L59.7642 163.038 L65.175 163.038 L65.175 186.655 L79.2432 186.655 L79.2432 164.025 L84.6541 164.025 L84.6541 186.655 L101.873 186.655 L101.873 162.465 L107.284 162.465 L107.284 193.084 L59.7642 193.084 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M77.1107 131.496 Q76.5378 132.483 76.2832 133.661 Q75.9967 134.806 75.9967 136.207 Q75.9967 141.172 79.2432 143.846 Q82.4579 146.487 88.5053 146.487 L107.284 146.487 L107.284 152.376 L71.6362 152.376 L71.6362 146.487 L77.1744 146.487 Q73.9279 144.641 72.3683 141.681 Q70.7768 138.721 70.7768 134.488 Q70.7768 133.883 70.8723 133.151 Q70.936 132.419 71.0951 131.528 L77.1107 131.496 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M77.1107 105.842 Q76.5378 106.829 76.2832 108.007 Q75.9967 109.153 75.9967 110.553 Q75.9967 115.518 79.2432 118.192 Q82.4579 120.834 88.5053 120.834 L107.284 120.834 L107.284 126.722 L71.6362 126.722 L71.6362 120.834 L77.1744 120.834 Q73.9279 118.988 72.3683 116.028 Q70.7768 113.068 70.7768 108.834 Q70.7768 108.23 70.8723 107.498 Q70.936 106.765 71.0951 105.874 L77.1107 105.842 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M75.7421 87.3182 Q75.7421 92.0289 79.4342 94.7661 Q83.0945 97.5034 89.492 97.5034 Q95.8895 97.5034 99.5817 94.798 Q103.242 92.0607 103.242 87.3182 Q103.242 82.6395 99.5498 79.9022 Q95.8577 77.1649 89.492 77.1649 Q83.1581 77.1649 79.466 79.9022 Q75.7421 82.6395 75.7421 87.3182 M70.7768 87.3182 Q70.7768 79.6794 75.7421 75.3189 Q80.7073 70.9584 89.492 70.9584 Q98.2449 70.9584 103.242 75.3189 Q108.207 79.6794 108.207 87.3182 Q108.207 94.9889 103.242 99.3494 Q98.2449 103.678 89.492 103.678 Q80.7073 103.678 75.7421 99.3494 Q70.7768 94.9889 70.7768 87.3182 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M77.1107 40.594 Q76.5378 41.5806 76.2832 42.7583 Q75.9967 43.9041 75.9967 45.3046 Q75.9967 50.2698 79.2432 52.9434 Q82.4579 55.5852 88.5053 55.5852 L107.284 55.5852 L107.284 61.4735 L71.6362 61.4735 L71.6362 55.5852 L77.1744 55.5852 Q73.9279 53.7391 72.3683 50.7791 Q70.7768 47.819 70.7768 43.5858 Q70.7768 42.9811 70.8723 42.249 Q70.936 41.517 71.0951 40.6258 L77.1107 40.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip202)\" cx=\"478.104\" cy=\"761.771\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"386.453\" cy=\"761.562\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"844.708\" cy=\"653.253\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"1028.01\" cy=\"68.0556\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"936.359\" cy=\"434.508\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"753.057\" cy=\"702.144\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"661.406\" cy=\"742.356\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"569.755\" cy=\"760.816\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"294.802\" cy=\"761.125\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "<circle clip-path=\"url(#clip202)\" cx=\"203.152\" cy=\"761.021\" r=\"14.4\" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"2\"/>\n",
       "</svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plots.plot(tuned_lasso_machine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this doesn't work with `Makie` but we can take the convenience. \n",
    "\n",
    "`report` also gets us some useful information too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = LassoRegressor(lambda = 0.02154434690031884, …),\n",
       " best_history_entry = (model = LassoRegressor(lambda = 0.02154434690031884, …),\n",
       "                       measure = [RootMeanSquaredError()],\n",
       "                       measurement = [5.00501639997187],\n",
       "                       per_fold = [[5.195550421954174, 5.594465814013773, 4.114753673922431, 4.854588100919187, 5.14401890179805]],),\n",
       " history = NamedTuple{(:model, :measure, :measurement, :per_fold), Tuple{MLJLinearModels.LassoRegressor, Vector{RootMeanSquaredError}, Vector{Float64}, Vector{Vector{Float64}}}}[(model = LassoRegressor(lambda = 0.02154434690031884, …), measure = [RootMeanSquaredError()], measurement = [5.00501639997187], per_fold = [[5.195550421954174, 5.594465814013773, 4.114753673922431, 4.854588100919187, 5.14401890179805]]), (model = LassoRegressor(lambda = 0.007742636826811269, …), measure = [RootMeanSquaredError()], measurement = [5.00631391933102], per_fold = [[5.173141601266498, 5.583597171707226, 4.117843599890676, 4.876589616915048, 5.161412760748969]]), (model = LassoRegressor(lambda = 1.291549665014884, …), measure = [RootMeanSquaredError()], measurement = [5.67902723720285], per_fold = [[6.145369593492875, 6.098863598206641, 5.196840803974826, 5.018965631235667, 5.839338731818849]]), (model = LassoRegressor(lambda = 10.000000000000002, …), measure = [RootMeanSquaredError()], measurement = [9.313715174414954], per_fold = [[9.973453424001466, 8.781991133737549, 9.173940857089404, 8.922473856481114, 9.662379608973254]]), (model = LassoRegressor(lambda = 3.593813663804628, …), measure = [RootMeanSquaredError()], measurement = [7.037660609843977], per_fold = [[7.512438084636223, 7.063395454670881, 6.8661160065870375, 6.433322821349736, 7.265250266341788]]), (model = LassoRegressor(lambda = 0.46415888336127803, …), measure = [RootMeanSquaredError()], measurement = [5.375361457189756], per_fold = [[5.877170727397991, 5.967933998434739, 4.578944338203925, 4.857603090906311, 5.4545524992795205]]), (model = LassoRegressor(lambda = 0.1668100537200059, …), measure = [RootMeanSquaredError()], measurement = [5.125607287752084], per_fold = [[5.524998247530005, 5.773465581399223, 4.2636201059791485, 4.717550048386918, 5.202595988478275]]), (model = LassoRegressor(lambda = 0.05994842503189412, …), measure = [RootMeanSquaredError()], measurement = [5.0109484736056835], per_fold = [[5.259321427353059, 5.623369801398444, 4.123251164019081, 4.795490781061791, 5.12516743466273]]), (model = LassoRegressor(lambda = 0.002782559402207125, …), measure = [RootMeanSquaredError()], measurement = [5.009033242868468], per_fold = [[5.167453566111362, 5.579935696829209, 4.1218990712709935, 4.889586604591623, 5.168723467607311]]), (model = LassoRegressor(lambda = 0.0010000000000000002, …), measure = [RootMeanSquaredError()], measurement = [5.009675758229785], per_fold = [[5.1655412989846035, 5.5784343387348025, 4.123519354099901, 4.892197295073789, 5.171605475316245]])],\n",
       " best_report = nothing,\n",
       " plotting = (parameter_names = [\"lambda\"],\n",
       "             parameter_scales = [:log],\n",
       "             parameter_values = Any[0.02154434690031884; 0.007742636826811269; … ; 0.002782559402207125; 0.0010000000000000002;;],\n",
       "             measurements = [5.00501639997187, 5.00631391933102, 5.67902723720285, 9.313715174414954, 7.037660609843977, 5.375361457189756, 5.125607287752084, 5.0109484736056835, 5.009033242868468, 5.009675758229785],),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(tuned_lasso_machine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a `NamedTuple` we can get specific things out, like the RMSE for each value of $\\lambda$ in our search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 5.00501639997187\n",
       " 5.00631391933102\n",
       " 5.67902723720285\n",
       " 9.313715174414954\n",
       " 7.037660609843977\n",
       " 5.375361457189756\n",
       " 5.125607287752084\n",
       " 5.0109484736056835\n",
       " 5.009033242868468\n",
       " 5.009675758229785"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(tuned_lasso_machine).plotting.measurements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get the best history entry RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(model = LassoRegressor(lambda = 0.02154434690031884, …),\n",
       " measure = [RootMeanSquaredError()],\n",
       " measurement = [5.00501639997187],\n",
       " per_fold = [[5.195550421954174, 5.594465814013773, 4.114753673922431, 4.854588100919187, 5.14401890179805]],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(tuned_lasso_machine).best_history_entry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there under `measurement` the measured RMSE in the grid search at that `lambda` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso RMSE: 4.6343333341600985\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(tuned_lasso_machine, X_test_standardized)\n",
    "best_lasso_rmse = rms(y_pred, y_test)\n",
    "println(\"Best Lasso RMSE: $best_lasso_rmse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see we got a minor improvement in test RMSE compared to linear regression but still not as good as the default random forest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up the steps as a `Pipeline`\n",
    "\n",
    "So in the above lasso model, we applied the following steps to the `boston` dataset:\n",
    "\n",
    "1. Updated datatypes to be `Continuous` type\n",
    "2. Applied a `Standardizer` to the features\n",
    "3. Used a `Grid` and `TunedModel` to find an optimal parameter value of $\\lambda$\n",
    "4. Calculate the test RMSE with the best model from step 3\n",
    "\n",
    "Reference:\n",
    "* [Composing Models](https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap the steps in a single code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso RMSE: 4.6343333341600985\n"
     ]
    }
   ],
   "source": [
    "# Load in boston housing market data\n",
    "boston = load_boston() |> DataFrame\n",
    "\n",
    "# Train Test Split with `partition`\n",
    "train, test = partition(boston, 0.8, rng=42);\n",
    "\n",
    "# As before, unpack to horizontally split into y and X\n",
    "y_train, X_train = unpack(train, ==(:MedV))\n",
    "y_test, X_test = unpack(test, ==(:MedV));\n",
    "\n",
    "# 1. Coerce datatypes\n",
    "X_train = coerce(X_train, :Chas=>Continuous)\n",
    "X_test = coerce(X_test, :Chas=>Continuous)\n",
    "\n",
    "# 2. Standardize\n",
    "Standardizer = @load Standardizer pkg=MLJModels verbosity=0\n",
    "# Just like the other models, we instantiate it and bind data to it with a machine\n",
    "standardizer = Standardizer()\n",
    "standardizer_machine = machine(standardizer, X_train) |> fit!\n",
    "\n",
    "# And transform\n",
    "X_train_standardized = MLJ.transform(standardizer_machine, X_train)\n",
    "X_test_standardized = MLJ.transform(standardizer_machine, X_test)\n",
    "\n",
    "# 3. Load and Tune Model\n",
    "# Load in our model and instantiate it\n",
    "LassoRegressor = @load LassoRegressor pkg=MLJLinearModels verbosity=0\n",
    "lasso = LassoRegressor()\n",
    "\n",
    "parameter_range = range(lasso, :lambda, lower=0.001, upper=10.0, scale=:log)\n",
    "tuned_lasso = TunedModel(lasso, \n",
    "                         resampling=CV(nfolds=5, rng=42), \n",
    "                         tuning=Grid(resolution=10), # Search over 10 values between 'lower' and 'upper' in `parameter_range`\n",
    "                         range=parameter_range, \n",
    "                         measure=rms)\n",
    "\n",
    "# As before we then need to take this model and bind it to data as a machine,\n",
    "# note that we don't pipe it with `|>` into fit! this time.\n",
    "tuned_lasso_machine = machine(tuned_lasso, X_train_standardized, y_train)\n",
    "# Here we call fit! separately so we can set verbosity to 0\n",
    "fit!(tuned_lasso_machine, verbosity=0)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred = predict(tuned_lasso_machine, X_test_standardized)\n",
    "best_lasso_rmse = rms(y_pred, y_test)\n",
    "println(\"Best Lasso RMSE: $best_lasso_rmse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using the julia base pipe `|>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicPipeline(\n",
       "  f = var\"#17#18\"(), \n",
       "  standardizer = Standardizer(\n",
       "        features = Symbol[], \n",
       "        ignore = false, \n",
       "        ordered_factor = false, \n",
       "        count = false), \n",
       "  deterministic_tuned_model = DeterministicTunedModel(\n",
       "        model = LassoRegressor(lambda = 1.0, …), \n",
       "        tuning = Grid(goal = nothing, …), \n",
       "        resampling = CV(nfolds = 5, …), \n",
       "        measure = RootMeanSquaredError(), \n",
       "        weights = nothing, \n",
       "        class_weights = nothing, \n",
       "        operation = nothing, \n",
       "        range = NumericRange(0.001 ≤ lambda ≤ 10.0; origin=5.0005, unit=4.9995; on log scale), \n",
       "        selection_heuristic = MLJTuning.NaiveSelection(nothing), \n",
       "        train_best = true, \n",
       "        repeats = 1, \n",
       "        n = nothing, \n",
       "        acceleration = CPU1{Nothing}(nothing), \n",
       "        acceleration_resampling = CPU1{Nothing}(nothing), \n",
       "        check_measure = true, \n",
       "        cache = true), \n",
       "  cache = true)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in boston housing market data\n",
    "boston = load_boston() |> DataFrame\n",
    "\n",
    "# Train Test Split with `partition`\n",
    "train, test = partition(boston, 0.8, rng=42);\n",
    "\n",
    "# As before, unpack to horizontally split into y and X\n",
    "y_train, X_train = unpack(train, ==(:MedV))\n",
    "y_test, X_test = unpack(test, ==(:MedV));\n",
    "\n",
    "# Specify the tuned_lasso model\n",
    "parameter_range = range(lasso, :lambda, lower=0.001, upper=10.0, scale=:log)\n",
    "tuned_lasso = TunedModel(lasso, \n",
    "                         resampling=CV(nfolds=5, rng=42), \n",
    "                         tuning=Grid(resolution=10), # Search over 10 values between 'lower' and 'upper' in `parameter_range`\n",
    "                         range=parameter_range, \n",
    "                         measure=rms)\n",
    "\n",
    "# Return a `DeterministicPipeline` model\n",
    "pipe = (X_train -> coerce(X_train, :Chas=>Continuous)) |> Standardizer() |> tuned_lasso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline is just another model, so we can bind it to a `machine` and `evaluate`, `predict`, etc. \n",
    "\n",
    "Here we bind a `machine` and `predict` on the `X_test` data to get predictions and calculate the RMSE on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6343333341600985"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe_machine = machine(pipe, X_train, y_train)\n",
    "fit!(pipe_machine, verbosity=0)\n",
    "\n",
    "y_pred_pipeline = predict(pipe_machine, X_test)\n",
    "rms(y_pred_pipeline, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using a `Pipeline()` \n",
    "\n",
    "For clarity, it may be helpful to use a `Pipeline` method instead in which we create the model with `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a `Pipeline` function instead of using the base `|>` pipes\n",
    "pipe = Pipeline(\n",
    "    transformer = (X_train -> coerce(X_train, :Chas=>Continuous)),\n",
    "    standardizer = Standardizer(),\n",
    "    tuner = tuned_lasso\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the same pipeline as before, and we can get our predictions out from a new `machine` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6343333341600985"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe_machine = machine(pipe, X_train, y_train)\n",
    "fit!(pipe_machine, verbosity=0)\n",
    "\n",
    "y_pred_pipeline = predict(pipe_machine, X_test)\n",
    "rms(y_pred_pipeline, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
